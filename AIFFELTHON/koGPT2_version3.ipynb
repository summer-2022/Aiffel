{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wyn2Nlz-HAkV"
   },
   "source": [
    "## KoGPT2\n",
    " Day04-1104 ~ Day06-1106\n",
    "\n",
    "* MODEL3 : SKT koGPT 2\n",
    " - Base Model(110M) 까지는 LMS 환경에서 사용 가능하다고 들어 적합한 사이즈의 모델을 서칭하여 선정\n",
    " - dataset 구성 : 최종  train 14000, val 3500, test 3500 참고 할것\n",
    "\n",
    " - [Huggingface_GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)\n",
    " - [SKT-koGPT2](https://github.com/SKT-AI/KoGPT2#model)\n",
    " - [GPT2_finetuning](https://hyyoka-ling-nlp.tistory.com/9)\n",
    " - [GPT2_finetuning_NonEng](https://www.philschmid.de/fine-tune-a-non-english-gpt-2-model-with-huggingface)\n",
    " - [GPT2 text summarization finetuning](https://towardsdatascience.com/text-summarization-with-gpt2-and-layer-ai-599625085d8e)\n",
    "\n",
    "* 추상요약\n",
    " - [Abstractive Summarization](https://medium.com/search?q=abstractive+text+summarization)\n",
    "\n",
    "\n",
    " GPT2 모델 학습\n",
    "  1. Data 로드\n",
    "   * 데이터 형식 변경 : train/validation/ test 하나로 취합하여 로드  \n",
    "   * 데이터 구성 : train/validation/test 형식에 맞추어 변경 후 데이터 다운로드)\n",
    "\n",
    "2. GPT Tokenizer, Model 허깅페이스 로드 \n",
    " * 4단계.trainer에서 dimension mismatch 오류가 지속적으로 발생하여 학습불가 \n",
    "    * 2단계.토크나이징 단계에 동적 패딩 적용하여 디멘젼 오류 해결\n",
    "\n",
    "3. Load Train Config \n",
    " * 5.test dataset evaluation단계 까지 완료 한 후 다시 3.Load Train Config 단계로 돌아와 파인튜닝하여 성능 향상 최종 목표!)\n",
    "\n",
    "4. Trainer : \n",
    " * traindatat 학습 예상 시간이 너무 길어서 학습 불가\n",
    "    * max step size 설정하여 train data학습양과 시간 줄여서 학습 성공\n",
    " * validation dataset의 크기가 너무 커서 현재 trainer 완료 하지 못한 상태\n",
    "    * 해결책 찾는중\n",
    "    \n",
    "5. Test evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58gxx9-uly4A"
   },
   "outputs": [],
   "source": [
    "! pip uninstall -y transformers\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJbHAry3ytWW"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y datasets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFSxAutmuHKv"
   },
   "source": [
    "# PROJECT : 뉴스 데이터셋 요약\n",
    "* Task : summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4TefdcwuXGd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification, AutoConfig\n",
    "from dataclasses import asdict\n",
    "from transformers.data.processors.utils import DataProcessor, InputExample, InputFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mvSBk5MCoylr"
   },
   "source": [
    "## (1) Dataset Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtPAk2Spxuhi"
   },
   "source": [
    ">Dataset dictionary\n",
    "* train dataset, validation dataset, test dataset으로 구성\n",
    "* Dataset은 ‘sentence1’, ‘sentence2’, ‘label’, ‘idx’(인덱스)로 구성 \n",
    "\n",
    " Huggingface datasets를 사용하면 DataProcessor를 사용할 필요없이 바로 사용가능하게 구성되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNt8TMBoyBN0"
   },
   "outputs": [],
   "source": [
    "# import datasets\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# huggingface_mrpc_dataset = load_dataset('glue', 'mrpc')\n",
    "# print(huggingface_mrpc_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhjgBpfMpkY6"
   },
   "source": [
    ">News Dataset dictionary\n",
    "* train dataset, validation dataset 으로 구성\n",
    "* Dataset은 ‘input’, target'로 구성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HVmwA175c-v_"
   },
   "outputs": [],
   "source": [
    "# Load a CSV file\n",
    "from datasets import load_dataset, Dataset\n",
    "# train_Sum1 = load_dataset('csv', data_files='/content/drive/MyDrive/Jiwon/Data/train_Sum1.csv')\n",
    "# val_Sum1 = load_dataset('csv', data_files='/content/drive/MyDrive/Jiwon/Data/val_Sum1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd AIFFELTHON_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k08QU1S7g8Bg"
   },
   "source": [
    "\n",
    "* 데이터 로드할때 형식 맞춰주기 \n",
    "```\n",
    "data_files = {\"train\": \"/content/drive/MyDrive/Jiwon/Data/train_Sum1.csv\", \"test\": \"/content/drive/MyDrive/Jiwon/Data/val_Sum1.csv\"}\n",
    "dataset = load_dataset('csv', data_files=data_files)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "90e9236dcfc34195835948daf3a5d6b1",
      "7b41892ccc7743a2afcd550a8e3b57d1",
      "d8d0841a119b434a841d897e13adaf92",
      "3080a75425b14f37b5881f23f1b2035e",
      "27a07962ce3a4ab280746f9cee032621",
      "c13a315ca033475ba62823805a462486",
      "8c36d30ca30b4eeb85842e02c5933194",
      "a20b879929f14131ba4a5e5a9fc05807",
      "2538659792a04f43830275f928f61e78",
      "585d1b3cd1a3406cbe36c7e26362e0cb",
      "7d39bc464c2e445da1418d69c8bdf4ca"
     ]
    },
    "id": "NzSfCNB56i7G",
    "outputId": "40983848-cbf3-4acb-d34f-c89663d859d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e14cee014c714360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /aiffel/.cache/huggingface/datasets/csv/default-e14cee014c714360/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c883c4728448cfb812564239643173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f14be45ea5d4a2486ce784fc7f5266c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /aiffel/.cache/huggingface/datasets/csv/default-e14cee014c714360/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc0eeab7dbd4592b6f75be6155b63a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# root = '/content/drive/MyDrive/Jiwon/Data/'\n",
    "root = '/aiffel/aiffel/Jiwon/AIFFELTHON_DATA/'\n",
    "\n",
    "data_files = {\n",
    "    \"train\" : root + \"train_Sum1_.csv\",\n",
    "    \"val\"   : root + \"val_Sum1.csv\",\n",
    "    \"test\"  : root + \"test_Sum1.csv\"\n",
    "    }\n",
    "dataset = load_dataset('csv', data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vJ7XTAB7Gu7"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJF1fkSlSuKo"
   },
   "outputs": [],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gESYOU7xafDD"
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# train_Sum1 = pd.read_csv(\"/content/drive/MyDrive/Jiwon/Data/train_Sum1.csv\")\n",
    "# val_Sum1 = pd.read_csv(\"/content/drive/MyDrive/Jiwon/Data/val_Sum1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IEWwFU_ka5U-"
   },
   "outputs": [],
   "source": [
    "# train_Sum1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jh0NEmkka5Xq"
   },
   "outputs": [],
   "source": [
    "# val_Sum1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adyYyWQEvl0c"
   },
   "source": [
    "## (2) Tokenizer, Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHUmRkrKvybs"
   },
   "source": [
    "> Model, Tokenizer 생성\n",
    "* Huggingface- Auto Class\n",
    " - Auto class는 from_pretrained 메소드를 이용해 pretrained model의 경로 혹은 이름만 안다면 자동으로 생성하는 방법\n",
    " - AutoTokenizer, AutoModel기능 지원\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjkXb6e8tbhI",
    "outputId": "72ab4c48-33c1-4347-e4c0-b7663321d79e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d895fa389758477097c890ac73b07636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/82.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8128f09e05584dd8b534918ddfdf71bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/344k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beba88a91ec94aa5b95e1ee6e7f39d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50314b41a1224612a6cdfa2c9f48779a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/526M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast, GPT2LMHeadModel\n",
    "tokenizer_gpt3 = BertTokenizerFast.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")\n",
    "input_ids = tokenizer_gpt3.encode(\"text to tokenize\")[1:]  # remove cls token\n",
    "        \n",
    "model_gpt3 = GPT2LMHeadModel.from_pretrained(\"kykim/gpt3-kor-small_based_on_gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Fy2V5uAUu5ZV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요',\n",
       " '.',\n",
       " '한국어',\n",
       " 'g',\n",
       " '##pt',\n",
       " '-',\n",
       " '2',\n",
       " '입니다',\n",
       " '.',\n",
       " '[UNK]',\n",
       " ':',\n",
       " ')',\n",
       " 'l',\n",
       " '^',\n",
       " 'o']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_gpt3.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rwd0G4v5u-qE",
    "outputId": "46c92438-0f48-490c-cc17-208b943d331b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer_gpt3.encode(text, return_tensors='pt')\n",
    "gen_ids = model_gpt3.generate(input_ids,\n",
    "                          max_length=128,\n",
    "                          repetition_penalty=2.0,\n",
    "                          pad_token_id=tokenizer_gpt3.pad_token_id,\n",
    "                          eos_token_id=tokenizer_gpt3.eos_token_id,\n",
    "                          bos_token_id=tokenizer_gpt3.bos_token_id,\n",
    "                          use_cache=True)\n",
    "generated = tokenizer_gpt3.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x_vSigOUs6vT"
   },
   "outputs": [],
   "source": [
    "# from transformers import GPT2TokenizerFast\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "# bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "# pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6p2GIOt9tYSm",
    "outputId": "bc4928c4-1d3c-43d3-801f-928bf4edbe34"
   },
   "outputs": [],
   "source": [
    "# tokenizer.tokenize(\"안녕하세요. 한국어 GPT-2 입니다.😤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08e-yY2qlT8B",
    "outputId": "cb4ac105-ea49-4e0a-c916-823d7debff6c"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import GPT2LMHeadModel\n",
    "\n",
    "# model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "# text = '근육이 커지기 위해서는'\n",
    "# input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "# gen_ids = model.generate(input_ids,\n",
    "#                           max_length=128,\n",
    "#                           repetition_penalty=2.0,\n",
    "#                           pad_token_id=tokenizer.pad_token_id,\n",
    "#                           eos_token_id=tokenizer.eos_token_id,\n",
    "#                           bos_token_id=tokenizer.bos_token_id,\n",
    "#                           use_cache=True)\n",
    "# generated = tokenizer.decode(gen_ids[0])\n",
    "# print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcVbM41x8kv6",
    "outputId": "dc8e51ed-3f45-4c26-fbd2-3c9089a5f6db"
   },
   "outputs": [],
   "source": [
    "gen_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78ULx9mSv_Du"
   },
   "source": [
    "> 토크나이징\n",
    "* `transform()` 함수를 만들고 이전에 만들어두었던 Tokenizer에서 입력할 `dataset의 형태를 확인하고 바꿀 대상을 지정`해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list | grep transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ue-43Cg-yyva",
    "outputId": "7c86a1fe-1e10-4d49-e0de-9490645301a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': ['4일 오후 4시 충남 아산시 신리초등학교 체육관 2층 입구.\\n  손 소독과 발열 체크를 한 뒤 체육관 안으로 들어가라는 안내문이 붙었다.\\n  마스크를 착용하지 않으면 출입이 통제됐다.\\n  체육관으로 들어가자 왼쪽에 설치된 대형 칠판에 ‘사람 두 팔 간격 건강 거리 유지’라는 글이 크게 쓰여 있었다.\\n  지난 3일 충남 아산지역에 폭우가 내리면서 온양천이 범람하자 신동·모종동 주민들이 급히 신리초로 대피했다.\\n  신리초는 홍수와 지진 등 각종 재난사고 때 임시 주거시설로 지정된 곳이다.\\n  3일 오후 5시까지 60여 명 수준이던 이재민은 오후 7시가 지나서는 100여 명을 넘어섰다.\\n  아산시는 주민들이 한꺼번에 몰리자 수해와 더불어 신종 코로나바이러스 감염증(코로나19)이 확산할 것을 우려했다.\\n  가장 먼저 주민들에게 마스크 착용을 의무화했다.\\n  마스크를 쓰지 않으면 체육관 출입을 제한하고 체육관에 머무는 시간에도 밥을 먹거나 물을 마실 때를 제외하고 마스크를 벗지 못하게 했다.\\n  이재민들은 집이나 농경지로 오갈 때도 출입 횟수에 관계없이 매번 발열 체크를 했다.\\n  외부에서 다른 사람과 접촉할 가능성이 있어서다.\\n  4일 오후 4시 30분쯤 체육관 안에는 주민 5명과 아산시청 공무원·자원봉사자 10여 명이 머물고 있었다.\\n  주민들은 아산시가 제공한 매트리스 위에서 쉬거나 대화를 나눴다.\\n  매트리스 1개마다 1명씩 배정됐다.\\n  매트리스 간격은 2~3m가량으로 1개의 매트리스에 2명 이상이 앉지 못하도록 했다.\\n  주민 간 접촉을 최소화하기 위해서다.\\n  다른 대피소와 달리 신리초 체육관에는 소형 천막(텐트)이 지급되지 않았다.\\n  이 때문에 충남도와 아산시 등 방역 당국이 바짝 긴장했다.\\n  이곳에 머무르는 주민들은 대부분 70~80대 고령자로 코로나19에 노출될 경우 감염 위험이 높아서다.\\n  아산시는 주민들이 식사를 할 때 밀접 접촉할 가능성이 크다고 판단, 모든 식사를 도시락으로 대체했다.\\n  한 곳에 도시락을 쌓아 놓고 주민들이 줄을 서서 받지 않고 공무원들이 일일이 매트리스로 이동하며 도시락을 나눠줬다.\\n  물도 각각 생수병을 전달하고 컵 사용도 자제할 것을 당부했다.\\n  아산시 관계자는 “이재민 대피시설에서 코로나19 확산 가능성이 있기 때문에 방역과 함께 철저한 관리가 이뤄지고 있다”며 “마을회관과 경로당 등 소규모 인원이 모여 있는 곳도 담당 공무원이 수시로 확인한다”고 말했다.\\n  지난 3일 새벽부터 4일 오후까지 내린 비로 충남에서는 620여 명(364가구)의 이재민이 발생했다.\\n ', ' “집안의 가보로 간직할 수 있게 장군님과 기념촬영을 하고 싶습니다.”\\n 그러자 김정일은 “오늘 예감이 왠지 실패할 것 같아 사진사를 안 데리고 왔는데 과학자들의 요구이니 사진사를 부르라”고 하면서, 사진사가 올 때까지 과학자들과 얘기를 나누겠다고 했다. 1시간가량 연구소 안팎을 돌아본 후, 사진사가 오자 김정일은 연구사들과 사진을 찍었다. 이때 ‘청년장군’도 함께 사진을 찍으라고 말했다.\\n 사진 촬영 후 연구사들의 대부분이 국방대학 졸업생들이라는 얘기를 듣자 김정일은 중앙당 간부들에게, “국방대학 졸업생들은 나라의 보배”라고 하면서 “잘 돌봐 주어야 한다”고 했다. 그 말을 듣고 연구사들이 감개무량해 눈물을 흘리며 ‘김정일 만세’를 부르는 장면이 북한 TV에 방영되기도 했다. 발사시험에 참여한 많은 사람들에게 훈장과 영웅칭호, 학위, 학직이 수여됐다.\\n 북한에서는 지대공 미사일에는 100-1로 시작하는 일련번호가 붙는다. 해상 대 해상 미사일은 200-1로 시작하는 일련번호로 통용된다. 지상 대 지상 단거리 미사일은 500-1로 시작한다. 중거리와 장거리 미사일은 900-1로 시작되며 짝수가 아니고 홑수로만 나간다.\\n 북한 미사일 본체에 새겨진 글자 ‘ㅈ’은 조선의 약호다. ‘ㅈ’ 다음에 100이나 900등의 숫자를 붙여 표시한다.\\n 원래는 ‘MADE IN DPRK’라고 ‘원산지 표기’를 했는데, 지금은 그렇게 쓰지 않는다. 1980년대 이란-이라크전쟁 당시, 이란에 수출한 북한산 미사일이 이라크를 타격한 적이 있다. 이라크가 이에 대한 보복으로 북한유조선을 침몰시킨 일이 있었다. 이후 북한의 모든 무장장비의 표기를 변형하였다.⊙\\n', '위원장 설훈] \"존경하는 교육문화체육관광위원회 위원님 여러분!  다음 안건을 의결하기 전에 위원장이 한 가지 제안하고자 합니다.  냉ㆍ난방기기 가동 및 디지털 교과서 활성화 등으로 초중등학교의 전기 사용량 증가가 예상되는 상황에서 재정 부담을 완화하고 지속 가능한 교육환경 조성을 위하여 초중등학교 공공요금 등 운영경비의 현실화 및 교육용 전기요금 인하 방안을 정부에 촉구하는 결의안을 제안합니다.  위원님들 의견을 말씀하여 주시기 바랍니다.\"\\n신성범 위원] \"안건에 없던 거네요?\"\\n위원장 설훈] \"그래서 제가 위원장이 제안하는 형식으로 집어넣었습니다.\"\\n신성범 위원] \"제안자를 위원장으로 합니까?\"\\n위원장 설훈] \"예 제가 제안하고 한 분이 찬성을 해 주시면 안건으로 성립할 수 있습니다.\"\\n박홍근 위원] \"그것은 제가 아까 의사진행발언을 통해서 말씀을 드렸고 지난번 회의에서도 제가 결의안을 만들자고 제안을 했기 때문에……\"\\n위원장 설훈] \"박홍근 위원의 찬성이 있었습니다.  그러므로 지금 제가 제안하고 1인 이상이 찬성하는 위원이 있었으므로 위원회 의사일정 제17항으로 상정하도록 하겠습니다.  그러면 의사일정 제17항 교육용 전기요금 인하 촉구 결의안은 본 위원이 제안한 대로 의결하고자 하는데 이의 없으십니까?\"\\n신성범 위원] \"잠깐만요 이것 취지에는 공감하는데 너무 급박해 가지고……\"\\n김태년 위원] \"문제없어.\"\\n신성범 위원] \"아니 이렇게 우리 위원회 차원에서 결의안을 낼 경우에 그래도 무게가 실리고 해야 되는데 정부부처가 산업통상자원부인가……\"\\n', '저희가 할 수 있는 모든 방법을 다하겠다는 말씀을 드리고요. \\n이 문제도 그렇습니다. \\n지금 여당이 야당을 몰아붙이고 있고 이걸 뒤집어씌우기를 하고 있는데 민생 법안 같은 경우 먼저 협의를 했었을 때, 우리가 민생 법안 다 통과시켜주겠다, 그러니 선거법, 공수처법에 대해서는 올리지 마라고 요청을 했던 겁니다. \\n그런 것들 안 지킨 것 아닙니까, 이거? 지금 저는 여당이 국가를 운용하는 모든 책임은 정부 여당에 있는 겁니다. \\n국회를 야당이 거리로 안 나가게 하고 또 밤을 새워서 농성을 안하게 하는 것은 여당의 책임인 거예요. \\n여당이 안고 가야 되는 거예요. \\n옛날에 강창희 의장, 정의화 의장 있을 때 야당이 얘기하는 걸 다 들어줬습니다. \\n왜 지금 여당은 이렇게 하는지 지금 여당이 좀 돌아보시기 바랍니다.', '해양수산부장관후보자 윤진숙] \"아니 가지고 있었던 게 아니고요. 그때 당시에는 고모가 저희 집에 워낙 자주 놀러 오셨었습니다. 저희 어머니하고 워낙 친하셔 가지고 자주 오셨기 때문에 그 돈은 제가 처음에 제가 갖고 있던 돈을 그때 당시에 은행에서 뺐든지 하여튼 누구한테 받았든지 간에 그것을 제가 드렸지요 그날. 오신다 그러길래 제가 찾아 놨습니다. 그때 당시에는 인터넷이라든가 이런 것이 제대로 좀 안 될 때라서요.\"\\n위원장 최규성] \"고모를 돈을 줬다고요?\"\\n해양수산부장관후보자 윤진숙] \"예.\"\\n위원장 최규성] \"그런데 그러면 은행에서 돈을 뺀 거고요 그것은.\"\\n해양수산부장관후보자 윤진숙] \"은행이 아니고 아마 다른 데서 받았던 것 같아요. 은행인지 다른 데서 받았는지 지금 기억이 사실 안 나……\"\\n위원장 최규성] \"아니 돈이 후보자가 무슨 재벌의 딸도 아니고 돈 6000이 적은 돈이 아니잖아요.\"\\n해양수산부장관후보자 윤진숙] \"예 당연하지요.\"\\n위원장 최규성] \"돈을 집에다 갖고 있었던 건지 통장에서 뺀 건지 그러면 어느 통장에서 뺀 건지 그 당시 후보자 입장이 한 달에 몇천씩 버는 그런 입장이 아니잖아요.\"\\n해양수산부장관후보자 윤진숙] \"예 그렇지요.\"\\n위원장 최규성] \"그게 왜 기억이 안 납니까?\"\\n해양수산부장관후보자 윤진숙] \"제가 아까도 앞에도 말씀드렸지만 그게……\"\\n위원장 최규성] \"그다음에 2억 얼마인가 아파트 팔아 쓰고 나서 돈 그것 현금으로 받았습니까 다?\"\\n'], 'target': ['충남 아산지역에 폭우가 내려 온양천이 범람하자 주민들은 신리초로 대피했고 이에 아산시는 코로나19 확산이 걱정되어 마스크를 쓰지 않으면 출입을 제한했다. ', '북한은 미사일에 MADE IN DPRK라고 표기했었지만 이란-이라크 전쟁 당시 이라크가 보복으로 북한유조선을 침몰시킨 뒤 모든 무장장비 표기를 변형하였다.', '초중등학교 교육용 전기요금 인하 방안 및 공공요금 등에 대한 운영경비의 현실화 결의안을 제안했다.', '정부 여당은 야당이 거리로 나와 밤을 새워서 농성하는 것을 안하게 할 책임이 있다.', '최 위원장은 후보자가 재벌 딸도 아니고 한 달에 몇천씩 버는 입장도 아니면서 6000을 고모에게 줬는지 물었다.']}\n",
      "{'input_ids': [[2, 18319, 15153, 26803, 21931, 31312, 8118, 5226, 8027, 26807, 20806, 8400, 16873, 16426, 2016, 5039, 17478, 8289, 25583, 41061, 7653, 3363, 20806, 8400, 19460, 14333, 14118, 15740, 16448, 4579, 13993, 2016, 22654, 14515, 14055, 16332, 20809, 8018, 22903, 14909, 2016, 20806, 23707, 14333, 8158, 30029, 24106, 15692, 6838, 17928, 1, 14019, 3319, 7386, 36390, 14274, 16619, 14621, 1, 15351, 26416, 14649, 23078, 14299, 2016, 14149, 16445, 21931, 31312, 34377, 7491, 20703, 34150, 8044, 5687, 8231, 39546, 4424, 8178, 15329, 33842, 1, 4156, 8005, 8201, 30634, 29734, 5226, 8027, 39364, 37244, 13990, 2016, 5226, 8027, 37925, 7814, 18854, 32958, 3408, 15435, 27860, 18074, 3463, 21856, 21928, 15996, 8054, 35049, 22400, 2016, 16445, 15153, 25264, 13988, 15480, 8088, 4143, 27964, 8109, 23027, 26869, 15153, 23117, 8048, 28240, 8034, 27234, 27010, 15536, 18284, 2016, 31312, 14080, 30634, 21258, 4163, 30589, 5099, 8010, 8061, 16739, 32094, 15824, 37565, 20108, 8184, 2010, 20895, 2011, 5921, 19976, 8150, 14199, 16352, 13990, 2016, 14115, 14336, 15885, 14229, 15099, 34557, 20556, 8393, 13990, 2016, 22654, 19683, 16332, 20806, 8400, 20809, 8051, 16872, 13973, 20806, 18692, 41857, 17977, 8144, 18126, 37031, 15162, 18806, 26892, 20826, 22654, 4427, 8038, 21911, 14218, 2016, 23027, 8222, 14022, 20723, 8155, 2944, 8217, 15050, 5683, 8194, 15966, 20809, 7858, 17842, 29831, 16166, 25583, 41061, 14218, 2016, 33551, 14044, 23173, 20897, 8150, 16632, 14110, 8013, 2016, 18319, 15153, 26803, 16554, 8779, 20806, 8400, 21972, 15885, 33197, 8289, 31312, 26776, 21476, 1, 33957, 8158, 23745, 19885, 19878, 8014, 14299, 2016, 35443, 31312, 15972, 14493, 8112, 18842, 17298, 5152, 14108, 22491, 39352, 8013, 2016, 18842, 16712, 14304, 27835, 8834, 29190, 14909, 2016, 18842, 36390, 8078, 2020, 2070, 26208, 25994, 13969, 16712, 8107, 18842, 8008, 27634, 19653, 5509, 8038, 36522, 14218, 2016, 15885, 2106, 20897, 8051, 18919, 14006, 41765, 2016, 14044, 37244, 22748, 15204, 5226, 8027, 8658, 20806, 18692, 8034, 18797, 6611, 8382, 2010, 22836, 2011, 5921, 17543, 15010, 15107, 2016, 5921, 14045, 21931, 17097, 31312, 8118, 3408, 33697, 3130, 17398, 27745, 20170, 13990, 2016, 23514, 33222, 14492, 35443, 14657, 16015, 2070, 16134, 8152, 25524, 15088, 20895, 8008, 17270, 8481, 14059, 20108, 23071, 25606, 8013, 2016, 31312, 14080, 30634, 20380, 7657, 3463, 4325, 8629, 20897, 8150, 16632, 30785, 16177, 2014, 14216, 20380, 19666, 13969, 15956, 13990, 2016, 7653, 16066, 19666, 8051, 21826, 15644, 30634, 22386, 18348, 18728, 14169, 21476, 13987, 24886, 18842, 8054, 14765, 14186, 19666, 8051, 16376, 20201, 2016, 26449, 15468, 26406, 19677, 16162, 13973, 6915, 31667, 24894, 8150, 14199, 36674, 2016, 31312, 8118, 15980, 1, 23027, 8222, 37244, 15996, 13970, 20895, 19976, 16632, 14946, 14045, 33697, 8289, 14057, 26051, 19946, 17540, 14069, 13984, 1, 4131, 1, 16260, 31371, 8289, 38348, 8098, 3408, 26777, 36844, 18218, 13979, 22246, 16532, 21476, 8018, 20039, 29305, 8013, 1, 2260, 14408, 2016, 14149, 16445, 17042, 14008, 18319, 15153, 13988, 21584, 24507, 21931, 14111, 26318, 8022, 8088, 4143, 2010, 16206, 8260, 16450, 2011, 5907, 23027, 21412, 36389, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1, 33395, 22036, 8054, 38478, 8150, 5099, 14756, 30531, 21037, 16022, 19550, 8051, 14042, 19236, 2016, 1, 24021, 22517, 16648, 1, 14098, 5669, 14356, 17268, 16265, 8150, 2190, 15565, 14070, 14245, 5508, 18516, 20968, 16331, 16926, 15493, 15550, 14070, 14245, 18058, 8042, 1, 2260, 15042, 2014, 14070, 14472, 5691, 17647, 16331, 30246, 19341, 16175, 14999, 14218, 2016, 17703, 25994, 35242, 27301, 8051, 14323, 8337, 7876, 2014, 14070, 14472, 5683, 8158, 22517, 16648, 14724, 8025, 14346, 15210, 33318, 2016, 15801, 1, 17894, 8020, 8176, 1, 3238, 14057, 15210, 6522, 25136, 14408, 2016, 14070, 15049, 7876, 14724, 31086, 23366, 21374, 17699, 41146, 32778, 8034, 19341, 3395, 8158, 22517, 16648, 15746, 8098, 30687, 14229, 2014, 1, 21374, 17699, 41146, 14022, 24470, 4485, 8232, 1, 15157, 15042, 1, 5957, 3246, 8485, 28815, 14094, 1, 2260, 14218, 2016, 2430, 15201, 16401, 14724, 21557, 2117, 8356, 8035, 8237, 8010, 24767, 7954, 17374, 1, 22517, 8066, 4023, 8016, 1, 3966, 20730, 25220, 15431, 15315, 8008, 22420, 22189, 14218, 2016, 21102, 19621, 8008, 26108, 14090, 19024, 7878, 16155, 21079, 9086, 8448, 2014, 7650, 8108, 2014, 7650, 8420, 8018, 35007, 14909, 2016, 35255, 14688, 6266, 8152, 8252, 29837, 13994, 14265, 2015, 26037, 21164, 5929, 8401, 16579, 8048, 28805, 8013, 2016, 23389, 3137, 23389, 29837, 8078, 14197, 2015, 26037, 21164, 5929, 8401, 16579, 8054, 7235, 8114, 14349, 2016, 20370, 3137, 20370, 3114, 14731, 29837, 8078, 15670, 2015, 26037, 22664, 2016, 6182, 14731, 8061, 27485, 29837, 8078, 21860, 2015, 26037, 14095, 17101, 6294, 14424, 15529, 7818, 16080, 8120, 39209, 2016, 15431, 29837, 35372, 35759, 29411, 1, 2095, 1, 5883, 35978, 5555, 8448, 8013, 2016, 1, 2095, 1, 16337, 14265, 14030, 21860, 20698, 35584, 15787, 16324, 14035, 2016, 20028, 1, 30951, 15373, 33749, 8085, 8274, 1, 15157, 1, 31786, 20651, 1, 3966, 14581, 2014, 15758, 14457, 19683, 15362, 2016, 33799, 21095, 2015, 38448, 22932, 14791, 2014, 21095, 8008, 16932, 8112, 15431, 8068, 29837, 8018, 38448, 8273, 25080, 8112, 15941, 13984, 2016, 38448, 8048, 15272, 14083, 4485, 32982, 15431, 8070, 24636, 8051, 37736, 16439, 14803, 14299, 2016, 14305, 22299, 14216, 27370, 22740, 8107, 20651, 8273, 22440, 15031, 2016, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 32244, 4964, 9128, 2037, 1, 27699, 13975, 14589, 15866, 19712, 19784, 17995, 17585, 8403, 15563, 2005, 14153, 5508, 21188, 38043, 14006, 14544, 32244, 8018, 7653, 14524, 18685, 17935, 14131, 2016, 1, 21907, 4340, 16551, 32533, 19731, 16203, 39297, 16059, 8107, 15027, 13978, 8237, 15275, 8048, 36628, 18049, 19776, 23079, 19449, 13973, 14893, 14873, 14589, 15253, 17037, 8051, 18132, 39297, 16059, 17706, 27069, 3408, 14703, 8217, 19875, 15573, 8393, 4340, 14589, 8114, 15027, 27069, 25740, 21774, 32610, 24161, 13975, 30755, 15555, 18685, 14015, 2016, 17585, 16027, 21927, 15025, 14033, 36292, 15981, 2016, 1, 34295, 8411, 17585, 2037, 1, 5508, 31843, 22482, 2173, 13982, 2033, 1, 32244, 4964, 9128, 2037, 1, 14116, 14137, 32244, 8018, 18685, 13975, 25283, 32745, 14236, 2016, 1, 34295, 8411, 17585, 2037, 1, 18685, 14454, 32244, 13969, 7666, 14073, 2033, 1, 32244, 4964, 9128, 2037, 1, 5669, 14137, 18685, 13973, 7653, 17880, 29855, 8051, 7677, 19856, 5508, 26947, 28217, 8150, 5099, 14041, 2016, 1, 4346, 8540, 8854, 17585, 2037, 1, 17036, 14137, 16720, 17545, 23725, 8310, 28251, 16329, 22997, 26094, 8014, 20469, 18444, 14217, 14137, 30755, 15555, 14016, 16995, 34750, 30771, 14045, 1, 1, 1, 32244, 4964, 9128, 2037, 1, 4346, 8540, 8854, 17585, 8107, 29855, 8018, 16302, 2016, 21227, 14136, 14137, 18685, 13973, 16490, 19653, 29855, 13975, 17585, 8018, 14091, 18883, 17585, 8113, 17545, 39400, 17825, 8240, 8200, 13969, 4902, 8104, 15865, 17058, 2016, 17031, 17545, 39400, 17825, 8240, 8200, 14589, 8114, 15027, 27069, 25740, 24161, 30755, 21103, 4489, 17585, 8018, 18685, 8112, 17532, 38043, 17935, 14696, 32364, 36953, 26748, 2033, 1, 34295, 8411, 17585, 2037, 1, 16537, 8120, 8055, 14372, 6778, 14948, 8034, 19129, 14476, 14000, 2446, 8288, 8010, 14352, 1, 1, 1, 25023, 8204, 17585, 2037, 1, 14162, 25289, 2016, 1, 34295, 8411, 17585, 2037, 1, 14010, 14063, 14020, 17585, 8113, 22379, 30755, 15555, 2814, 20661, 14364, 22489, 5230, 13997, 14449, 16214, 14573, 8210, 28269, 15666, 35206, 23718, 35788, 8048, 1, 1, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 19093, 7657, 5099, 13979, 14216, 16808, 3110, 20247, 22997, 19607, 8055, 2016, 5921, 30868, 38567, 2016, 14136, 36559, 8018, 28849, 8051, 21827, 8800, 14122, 14204, 16965, 21922, 9293, 20129, 8273, 14042, 14266, 40842, 40133, 14051, 14059, 14336, 41480, 7689, 16070, 3463, 2014, 15182, 40842, 40133, 3110, 17219, 14641, 8040, 14487, 2014, 20175, 16362, 8532, 2014, 37181, 8275, 20082, 17524, 15288, 8038, 21087, 8014, 35348, 15932, 17224, 2016, 14053, 22135, 5508, 6266, 8584, 2190, 32208, 2014, 14568, 2033, 14136, 14156, 36559, 8018, 39858, 22359, 13975, 14216, 16314, 8078, 14573, 36559, 8008, 13979, 17224, 2016, 18695, 8273, 28849, 8018, 16619, 8054, 5508, 15001, 8111, 14042, 3547, 29591, 4909, 14505, 2944, 14294, 5508, 13977, 14032, 14181, 36559, 8107, 16314, 8159, 16429, 2016, 36559, 8018, 20501, 17198, 14220, 16429, 2016, 32869, 2122, 8609, 8829, 35716, 2014, 18920, 8393, 35716, 14178, 3463, 28849, 8018, 15970, 13975, 2181, 3110, 14007, 33954, 2016, 5730, 14136, 36559, 8078, 14063, 21057, 14136, 36559, 8018, 6096, 14323, 33779, 8143, 15981, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 14010, 14352, 15946, 2199, 15529, 8055, 2016, 15337, 28056, 2260, 18026, 14325, 14662, 15490, 14392, 17874, 41046, 14236, 2016, 14325, 15810, 13973, 15490, 6834, 8052, 8203, 14352, 14392, 41046, 8143, 14045, 2430, 31958, 14137, 17108, 14137, 15095, 14858, 16830, 15337, 29196, 17370, 13970, 4708, 16014, 40625, 16115, 14747, 16113, 16014, 24531, 17915, 14137, 26094, 14383, 18120, 2016, 23465, 8013, 14092, 15426, 14137, 14292, 2958, 13974, 2016, 15337, 28056, 14982, 13981, 24751, 14119, 14060, 14666, 6096, 5508, 3295, 3463, 14430, 8055, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 2260, 18431, 16830, 35807, 14089, 2033, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 14447, 17031, 17370, 13970, 16830, 4702, 33058, 8055, 17036, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 17370, 8018, 15529, 15054, 14044, 31295, 26516, 2190, 14085, 2016, 17370, 14312, 14044, 31295, 16113, 14513, 14136, 17764, 14132, 5508, 2783, 1, 1, 1, 32244, 6708, 8234, 8238, 2037, 1, 14010, 19794, 38932, 8048, 15440, 29359, 8107, 3449, 8144, 15529, 3242, 18607, 8022, 8018, 16418, 19794, 14010, 14862, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 17444, 23772, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 16830, 14662, 8013, 15095, 15946, 21159, 7235, 14611, 4702, 21159, 17031, 14385, 7235, 14611, 4702, 21159, 2430, 14791, 38932, 29188, 7653, 27504, 4145, 8024, 8834, 4411, 8034, 14053, 29188, 14010, 14862, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 18004, 8055, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 16237, 5730, 17764, 5508, 2798, 14073, 2033, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 14137, 16720, 8144, 15786, 8144, 28742, 13976, 16237, 1, 1, 1, 32244, 6708, 8234, 8238, 2037, 1, 31935, 30051, 14332, 14674, 16277, 29110, 14616, 14924, 3242, 14507, 20285, 13969, 16113, 17328, 3110, 2033, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'labels': [[2, 18319, 15153, 26803, 21931, 31312, 8118, 5226, 8027, 26807, 20806, 8400, 16873, 16426, 2016, 5039, 17478, 8289, 25583, 41061, 7653, 3363, 20806, 8400, 19460, 14333, 14118, 15740, 16448, 4579, 13993, 2016, 22654, 14515, 14055, 16332, 20809, 8018, 22903, 14909, 2016, 20806, 23707, 14333, 8158, 30029, 24106, 15692, 6838, 17928, 1, 14019, 3319, 7386, 36390, 14274, 16619, 14621, 1, 15351, 26416, 14649, 23078, 14299, 2016, 14149, 16445, 21931, 31312, 34377, 7491, 20703, 34150, 8044, 5687, 8231, 39546, 4424, 8178, 15329, 33842, 1, 4156, 8005, 8201, 30634, 29734, 5226, 8027, 39364, 37244, 13990, 2016, 5226, 8027, 37925, 7814, 18854, 32958, 3408, 15435, 27860, 18074, 3463, 21856, 21928, 15996, 8054, 35049, 22400, 2016, 16445, 15153, 25264, 13988, 15480, 8088, 4143, 27964, 8109, 23027, 26869, 15153, 23117, 8048, 28240, 8034, 27234, 27010, 15536, 18284, 2016, 31312, 14080, 30634, 21258, 4163, 30589, 5099, 8010, 8061, 16739, 32094, 15824, 37565, 20108, 8184, 2010, 20895, 2011, 5921, 19976, 8150, 14199, 16352, 13990, 2016, 14115, 14336, 15885, 14229, 15099, 34557, 20556, 8393, 13990, 2016, 22654, 19683, 16332, 20806, 8400, 20809, 8051, 16872, 13973, 20806, 18692, 41857, 17977, 8144, 18126, 37031, 15162, 18806, 26892, 20826, 22654, 4427, 8038, 21911, 14218, 2016, 23027, 8222, 14022, 20723, 8155, 2944, 8217, 15050, 5683, 8194, 15966, 20809, 7858, 17842, 29831, 16166, 25583, 41061, 14218, 2016, 33551, 14044, 23173, 20897, 8150, 16632, 14110, 8013, 2016, 18319, 15153, 26803, 16554, 8779, 20806, 8400, 21972, 15885, 33197, 8289, 31312, 26776, 21476, 1, 33957, 8158, 23745, 19885, 19878, 8014, 14299, 2016, 35443, 31312, 15972, 14493, 8112, 18842, 17298, 5152, 14108, 22491, 39352, 8013, 2016, 18842, 16712, 14304, 27835, 8834, 29190, 14909, 2016, 18842, 36390, 8078, 2020, 2070, 26208, 25994, 13969, 16712, 8107, 18842, 8008, 27634, 19653, 5509, 8038, 36522, 14218, 2016, 15885, 2106, 20897, 8051, 18919, 14006, 41765, 2016, 14044, 37244, 22748, 15204, 5226, 8027, 8658, 20806, 18692, 8034, 18797, 6611, 8382, 2010, 22836, 2011, 5921, 17543, 15010, 15107, 2016, 5921, 14045, 21931, 17097, 31312, 8118, 3408, 33697, 3130, 17398, 27745, 20170, 13990, 2016, 23514, 33222, 14492, 35443, 14657, 16015, 2070, 16134, 8152, 25524, 15088, 20895, 8008, 17270, 8481, 14059, 20108, 23071, 25606, 8013, 2016, 31312, 14080, 30634, 20380, 7657, 3463, 4325, 8629, 20897, 8150, 16632, 30785, 16177, 2014, 14216, 20380, 19666, 13969, 15956, 13990, 2016, 7653, 16066, 19666, 8051, 21826, 15644, 30634, 22386, 18348, 18728, 14169, 21476, 13987, 24886, 18842, 8054, 14765, 14186, 19666, 8051, 16376, 20201, 2016, 26449, 15468, 26406, 19677, 16162, 13973, 6915, 31667, 24894, 8150, 14199, 36674, 2016, 31312, 8118, 15980, 1, 23027, 8222, 37244, 15996, 13970, 20895, 19976, 16632, 14946, 14045, 33697, 8289, 14057, 26051, 19946, 17540, 14069, 13984, 1, 4131, 1, 16260, 31371, 8289, 38348, 8098, 3408, 26777, 36844, 18218, 13979, 22246, 16532, 21476, 8018, 20039, 29305, 8013, 1, 2260, 14408, 2016, 14149, 16445, 17042, 14008, 18319, 15153, 13988, 21584, 24507, 21931, 14111, 26318, 8022, 8088, 4143, 2010, 16206, 8260, 16450, 2011, 5907, 23027, 21412, 36389, 3, 21931, 31312, 34377, 7491, 20703, 14680, 5687, 8231, 39546, 4424, 8178, 15329, 35443, 5226, 8027, 39364, 37244, 14943, 15272, 31312, 14080, 20895, 19976, 8018, 14572, 14062, 22654, 19683, 16332, 20809, 8051, 16872, 13990, 2016, 3], [2, 1, 33395, 22036, 8054, 38478, 8150, 5099, 14756, 30531, 21037, 16022, 19550, 8051, 14042, 19236, 2016, 1, 24021, 22517, 16648, 1, 14098, 5669, 14356, 17268, 16265, 8150, 2190, 15565, 14070, 14245, 5508, 18516, 20968, 16331, 16926, 15493, 15550, 14070, 14245, 18058, 8042, 1, 2260, 15042, 2014, 14070, 14472, 5691, 17647, 16331, 30246, 19341, 16175, 14999, 14218, 2016, 17703, 25994, 35242, 27301, 8051, 14323, 8337, 7876, 2014, 14070, 14472, 5683, 8158, 22517, 16648, 14724, 8025, 14346, 15210, 33318, 2016, 15801, 1, 17894, 8020, 8176, 1, 3238, 14057, 15210, 6522, 25136, 14408, 2016, 14070, 15049, 7876, 14724, 31086, 23366, 21374, 17699, 41146, 32778, 8034, 19341, 3395, 8158, 22517, 16648, 15746, 8098, 30687, 14229, 2014, 1, 21374, 17699, 41146, 14022, 24470, 4485, 8232, 1, 15157, 15042, 1, 5957, 3246, 8485, 28815, 14094, 1, 2260, 14218, 2016, 2430, 15201, 16401, 14724, 21557, 2117, 8356, 8035, 8237, 8010, 24767, 7954, 17374, 1, 22517, 8066, 4023, 8016, 1, 3966, 20730, 25220, 15431, 15315, 8008, 22420, 22189, 14218, 2016, 21102, 19621, 8008, 26108, 14090, 19024, 7878, 16155, 21079, 9086, 8448, 2014, 7650, 8108, 2014, 7650, 8420, 8018, 35007, 14909, 2016, 35255, 14688, 6266, 8152, 8252, 29837, 13994, 14265, 2015, 26037, 21164, 5929, 8401, 16579, 8048, 28805, 8013, 2016, 23389, 3137, 23389, 29837, 8078, 14197, 2015, 26037, 21164, 5929, 8401, 16579, 8054, 7235, 8114, 14349, 2016, 20370, 3137, 20370, 3114, 14731, 29837, 8078, 15670, 2015, 26037, 22664, 2016, 6182, 14731, 8061, 27485, 29837, 8078, 21860, 2015, 26037, 14095, 17101, 6294, 14424, 15529, 7818, 16080, 8120, 39209, 2016, 15431, 29837, 35372, 35759, 29411, 1, 2095, 1, 5883, 35978, 5555, 8448, 8013, 2016, 1, 2095, 1, 16337, 14265, 14030, 21860, 20698, 35584, 15787, 16324, 14035, 2016, 20028, 1, 30951, 15373, 33749, 8085, 8274, 1, 15157, 1, 31786, 20651, 1, 3966, 14581, 2014, 15758, 14457, 19683, 15362, 2016, 33799, 21095, 2015, 38448, 22932, 14791, 2014, 21095, 8008, 16932, 8112, 15431, 8068, 29837, 8018, 38448, 8273, 25080, 8112, 15941, 13984, 2016, 38448, 8048, 15272, 14083, 4485, 32982, 15431, 8070, 24636, 8051, 37736, 16439, 14803, 14299, 2016, 14305, 22299, 14216, 27370, 22740, 8107, 20651, 8273, 22440, 15031, 2016, 1, 3, 32468, 29837, 8008, 30951, 15373, 33749, 8085, 8274, 14046, 20651, 8270, 15258, 21095, 2015, 38448, 16243, 14791, 38448, 8048, 4485, 32982, 15431, 8070, 24636, 8051, 37736, 16439, 3363, 14216, 27370, 22740, 20651, 8273, 22440, 15031, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 32244, 4964, 9128, 2037, 1, 27699, 13975, 14589, 15866, 19712, 19784, 17995, 17585, 8403, 15563, 2005, 14153, 5508, 21188, 38043, 14006, 14544, 32244, 8018, 7653, 14524, 18685, 17935, 14131, 2016, 1, 21907, 4340, 16551, 32533, 19731, 16203, 39297, 16059, 8107, 15027, 13978, 8237, 15275, 8048, 36628, 18049, 19776, 23079, 19449, 13973, 14893, 14873, 14589, 15253, 17037, 8051, 18132, 39297, 16059, 17706, 27069, 3408, 14703, 8217, 19875, 15573, 8393, 4340, 14589, 8114, 15027, 27069, 25740, 21774, 32610, 24161, 13975, 30755, 15555, 18685, 14015, 2016, 17585, 16027, 21927, 15025, 14033, 36292, 15981, 2016, 1, 34295, 8411, 17585, 2037, 1, 5508, 31843, 22482, 2173, 13982, 2033, 1, 32244, 4964, 9128, 2037, 1, 14116, 14137, 32244, 8018, 18685, 13975, 25283, 32745, 14236, 2016, 1, 34295, 8411, 17585, 2037, 1, 18685, 14454, 32244, 13969, 7666, 14073, 2033, 1, 32244, 4964, 9128, 2037, 1, 5669, 14137, 18685, 13973, 7653, 17880, 29855, 8051, 7677, 19856, 5508, 26947, 28217, 8150, 5099, 14041, 2016, 1, 4346, 8540, 8854, 17585, 2037, 1, 17036, 14137, 16720, 17545, 23725, 8310, 28251, 16329, 22997, 26094, 8014, 20469, 18444, 14217, 14137, 30755, 15555, 14016, 16995, 34750, 30771, 14045, 1, 1, 1, 32244, 4964, 9128, 2037, 1, 4346, 8540, 8854, 17585, 8107, 29855, 8018, 16302, 2016, 21227, 14136, 14137, 18685, 13973, 16490, 19653, 29855, 13975, 17585, 8018, 14091, 18883, 17585, 8113, 17545, 39400, 17825, 8240, 8200, 13969, 4902, 8104, 15865, 17058, 2016, 17031, 17545, 39400, 17825, 8240, 8200, 14589, 8114, 15027, 27069, 25740, 24161, 30755, 21103, 4489, 17585, 8018, 18685, 8112, 17532, 38043, 17935, 14696, 32364, 36953, 26748, 2033, 1, 34295, 8411, 17585, 2037, 1, 16537, 8120, 8055, 14372, 6778, 14948, 8034, 19129, 14476, 14000, 2446, 8288, 8010, 14352, 1, 1, 1, 25023, 8204, 17585, 2037, 1, 14162, 25289, 2016, 1, 34295, 8411, 17585, 2037, 1, 14010, 14063, 14020, 17585, 8113, 22379, 30755, 15555, 2814, 20661, 14364, 22489, 5230, 13997, 14449, 16214, 14573, 8210, 28269, 15666, 35206, 23718, 35788, 8048, 1, 1, 1, 3, 39297, 16059, 14589, 8114, 15027, 27069, 25740, 24182, 4340, 17706, 27069, 15882, 14083, 14703, 8217, 19875, 15573, 8393, 30755, 15555, 40651, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 19093, 7657, 5099, 13979, 14216, 16808, 3110, 20247, 22997, 19607, 8055, 2016, 5921, 30868, 38567, 2016, 14136, 36559, 8018, 28849, 8051, 21827, 8800, 14122, 14204, 16965, 21922, 9293, 20129, 8273, 14042, 14266, 40842, 40133, 14051, 14059, 14336, 41480, 7689, 16070, 3463, 2014, 15182, 40842, 40133, 3110, 17219, 14641, 8040, 14487, 2014, 20175, 16362, 8532, 2014, 37181, 8275, 20082, 17524, 15288, 8038, 21087, 8014, 35348, 15932, 17224, 2016, 14053, 22135, 5508, 6266, 8584, 2190, 32208, 2014, 14568, 2033, 14136, 14156, 36559, 8018, 39858, 22359, 13975, 14216, 16314, 8078, 14573, 36559, 8008, 13979, 17224, 2016, 18695, 8273, 28849, 8018, 16619, 8054, 5508, 15001, 8111, 14042, 3547, 29591, 4909, 14505, 2944, 14294, 5508, 13977, 14032, 14181, 36559, 8107, 16314, 8159, 16429, 2016, 36559, 8018, 20501, 17198, 14220, 16429, 2016, 32869, 2122, 8609, 8829, 35716, 2014, 18920, 8393, 35716, 14178, 3463, 28849, 8018, 15970, 13975, 2181, 3110, 14007, 33954, 2016, 5730, 14136, 36559, 8078, 14063, 21057, 14136, 36559, 8018, 6096, 14323, 33779, 8143, 15981, 2016, 3, 14573, 36559, 8078, 28849, 8018, 16619, 8054, 14591, 29591, 4909, 14505, 2944, 8238, 13975, 14199, 5508, 13977, 7657, 36908, 13984, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 14010, 14352, 15946, 2199, 15529, 8055, 2016, 15337, 28056, 2260, 18026, 14325, 14662, 15490, 14392, 17874, 41046, 14236, 2016, 14325, 15810, 13973, 15490, 6834, 8052, 8203, 14352, 14392, 41046, 8143, 14045, 2430, 31958, 14137, 17108, 14137, 15095, 14858, 16830, 15337, 29196, 17370, 13970, 4708, 16014, 40625, 16115, 14747, 16113, 16014, 24531, 17915, 14137, 26094, 14383, 18120, 2016, 23465, 8013, 14092, 15426, 14137, 14292, 2958, 13974, 2016, 15337, 28056, 14982, 13981, 24751, 14119, 14060, 14666, 6096, 5508, 3295, 3463, 14430, 8055, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 2260, 18431, 16830, 35807, 14089, 2033, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 14447, 17031, 17370, 13970, 16830, 4702, 33058, 8055, 17036, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 17370, 8018, 15529, 15054, 14044, 31295, 26516, 2190, 14085, 2016, 17370, 14312, 14044, 31295, 16113, 14513, 14136, 17764, 14132, 5508, 2783, 1, 1, 1, 32244, 6708, 8234, 8238, 2037, 1, 14010, 19794, 38932, 8048, 15440, 29359, 8107, 3449, 8144, 15529, 3242, 18607, 8022, 8018, 16418, 19794, 14010, 14862, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 17444, 23772, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 16830, 14662, 8013, 15095, 15946, 21159, 7235, 14611, 4702, 21159, 17031, 14385, 7235, 14611, 4702, 21159, 2430, 14791, 38932, 29188, 7653, 27504, 4145, 8024, 8834, 4411, 8034, 14053, 29188, 14010, 14862, 2016, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 5669, 18004, 8055, 2016, 1, 32244, 6708, 8234, 8238, 2037, 1, 16237, 5730, 17764, 5508, 2798, 14073, 2033, 1, 21326, 26715, 20906, 8400, 28022, 8158, 5862, 8250, 8579, 2037, 1, 14137, 16720, 8144, 15786, 8144, 28742, 13976, 16237, 1, 1, 1, 32244, 6708, 8234, 8238, 2037, 1, 31935, 30051, 14332, 14674, 16277, 29110, 14616, 14924, 3242, 14507, 20285, 13969, 16113, 17328, 3110, 2033, 1, 3, 6708, 29853, 38932, 8048, 29359, 3449, 8144, 15529, 7653, 27504, 4145, 8024, 8834, 4411, 8034, 16204, 8144, 16216, 8044, 18607, 8022, 8051, 2260, 37525, 8111, 6199, 14513, 23042, 2016, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "def transform(data):\n",
    "  return tokenizer_gpt3(\n",
    "      text = data['input'],\n",
    "      text_target = data['target'],\n",
    "#       text_pair_target=data['target'],\n",
    "      truncation = True,\n",
    "      padding = 'max_length',\n",
    "      return_token_type_ids = False,\n",
    "      max_length=128\n",
    "      )\n",
    "  \n",
    "examples = dataset['train'][:5]\n",
    "examples_transformed = transform(examples)\n",
    "\n",
    "print(examples)\n",
    "print(examples_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_transformed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glxig0rHuX8s",
    "outputId": "c84f1baf-9e38-44f4-ef0e-ebd2f073ac14"
   },
   "outputs": [],
   "source": [
    "# def transform(data):\n",
    "#   return tokenizer(\n",
    "#       text = data['input'],\n",
    "#       text_target = data['target'],\n",
    "#       truncation = True,\n",
    "#       padding = 'max_length',\n",
    "#       return_token_type_ids = False,\n",
    "#       max_length=128\n",
    "#       )\n",
    "  \n",
    "# examples = dataset['train'][:5]\n",
    "# examples_transformed = transform(examples)\n",
    "\n",
    "# print(examples)\n",
    "# print(examples_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lc60iuA9Gpa_"
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# len(encoded_data['test']['input_ids'][0]), len(encoded_data['train']['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJYlWupbHmqA"
   },
   "outputs": [],
   "source": [
    "# len(encoded_data['test']['labels'][0]), len(encoded_data['train']['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWWRK2yMIA0A"
   },
   "outputs": [],
   "source": [
    "# len(encoded_data['test']['attention_mask'][0]), len(encoded_data['train']['attention_mask'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fu-ec_iWH3Qu"
   },
   "source": [
    "expected sequence of length 278 at dim 1 (got 281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLOexLprxiMz"
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tthQu5tAwMRi"
   },
   "source": [
    "> `map():`\n",
    "* Data dictionary에 있는 모든 데이터들이 한번에 토크나징 적용시킬 수 있습니다.\n",
    "* 우리는 map을 사용해 토크나이징을 진행하기 때문에 batch를 적용해야 되므로 batched=True 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mH9f2rMmvheF",
    "outputId": "401884d1-f267-4a61-fc94-98688d17ac20"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b4e5ea17ee4dd8a948bfb876f07f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9513d23a35574eff896204cee4c17d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91505412adb346d3a2d28c252ad2355e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w9DUkKxpASDQ",
    "outputId": "f1cd7d7d-1371-4584-83cd-41ae0f70e133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 15250\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 18300\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3050\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYp44akmwT0m"
   },
   "source": [
    "## (3) Train/Evaluation과 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qLhg3IDwbkK"
   },
   "source": [
    ">Trainer\n",
    "* 사용하기 위해서는 TrainingArguments를 통해 학습 관련 설정을 미리 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTcdHUgBzbkD",
    "outputId": "5c9d3f5a-5397-484e-f2d3-39c49ae5097b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
      "     |████████████████████████████████| 72 kB 1.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.6.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.21.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2021.11.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.3.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.10.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.3.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o_DmQWLM0n9G",
    "outputId": "a006be5a-85e2-4105-feb6-48896189f385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from rouge_score) (0.12.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (from rouge_score) (3.6.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.21.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (2021.11.10)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk->rouge_score) (4.62.3)\n",
      "Building wheels for collected packages: rouge-score\n",
      "  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=f04c96d915d85c69c9765936f38aa57f0659a661d6fe668dcf301d902abf8503\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
      "Successfully built rouge-score\n",
      "Installing collected packages: rouge-score\n",
      "Successfully installed rouge-score-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H_dSnOQm0kYe"
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_87SorJ5kok"
   },
   "outputs": [],
   "source": [
    "# def _rouge(guess, answers):\n",
    "#     global rouge\n",
    "#     \"\"\"Compute ROUGE score between guess and *any* answers. Return the best.\"\"\"\n",
    "#     if rouge is None:\n",
    "#         return None, None, None\n",
    "#     evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l'], max_n=2)\n",
    "#     try:\n",
    "#         scores = [\n",
    "#             evaluator.get_scores(normalize_answer(guess), normalize_answer(a))\n",
    "#             for a in answers\n",
    "#         ]\n",
    "#     except LookupError:\n",
    "#         warn_once(\n",
    "#             'ROUGE requires nltk punkt tokenizer. Please run '\n",
    "#             '`python -c \"import nltk; nltk.download(\\'punkt\\')`'\n",
    "#         )\n",
    "#         rouge = None\n",
    "#         return None, None, None\n",
    "\n",
    "#     scores_rouge1 = [score['rouge-1']['r'] for score in scores]\n",
    "#     scores_rouge2 = [score['rouge-2']['r'] for score in scores]\n",
    "#     scores_rougeL = [score['rouge-l']['r'] for score in scores]\n",
    "#     return max(scores_rouge1), max(scores_rouge2), max(scores_rougeL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrhUU4uHCrSg"
   },
   "outputs": [],
   "source": [
    "# scores = rouge.compute(\n",
    "#     predictions=[generated_summary], references=[reference_summary]\n",
    "# )\n",
    "# scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bDLlEmtkzyVv",
    "outputId": "a96947cf-6862-42e6-cbaf-e0eb5c596329"
   },
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "predictions = [\"hello there 2\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general kenobi\"]\n",
    "results = rouge.compute(predictions=predictions, references=references)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1UbOgZV05DW"
   },
   "outputs": [],
   "source": [
    "# def compute_rouge(eval_pred):    \n",
    "#     print('eval_pred 합니당')\n",
    "#     print(eval_pred)\n",
    "#     predictions,references = eval_pred\n",
    "    \n",
    "#     return rouge.compute(predictions=predictions, references = references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c6pNMSYES3-E",
    "outputId": "ed7b7661-473b-4587-dbb4-38b8b4426e22"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NloXbqcgTKzk"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvrUxgVyN8Ed",
    "outputId": "a934c168-4f9a-4599-c756-64b20f95c7cc"
   },
   "outputs": [],
   "source": [
    "\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "from math import log\n",
    "from numpy import array\n",
    "from numpy import argmax\n",
    "\n",
    "# def beam_search_decoder(data, k):\n",
    "#     sequences = [[list(), 0.0]]\n",
    "#     # walk over each step in sequence\n",
    "#     for row in data:\n",
    "#         all_candidates = list()\n",
    "#         # expand each current candidate\n",
    "#         for i in range(len(sequences)):\n",
    "#             seq, score = sequences[i]\n",
    "#             for j in range(len(row)):\n",
    "#                 candidate = [seq + [j], score - log(row[j])]\n",
    "#                 all_candidates.append(candidate)\n",
    "#         # order all candidates by score\n",
    "#         ordered = sorted(all_candidates, key=lambda tup:tup[1])\n",
    "#         # select k best\n",
    "#         sequences = ordered[:k]\n",
    "#     return sequences\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # np.where(a < 0)\n",
    "\n",
    "    # print(labels_ids.shape, pred_ids.shape, type(labels_ids), type(pred_ids), sep = '\\n\\n')\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    \n",
    "    # beam search\n",
    "    # result = []\n",
    "    # for i in range(pred_ids.shape[0]):\n",
    "    #     result.append(beam_search_decoder(pred_ids[i], 3))\n",
    "\n",
    "    # print(result)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "    pred_str = tokenizer_gpt3.batch_decode(np.argmax(pred_ids, axis =2), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer_gpt3.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRUZMdk7IFvW"
   },
   "outputs": [],
   "source": [
    "labels_ids = np.arange(6).reshape(2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1CkYNasyHuqL",
    "outputId": "f2a4e0c0-b147-483a-d682-3790a91f39bd"
   },
   "outputs": [],
   "source": [
    "pred_ids = np.arange(24).reshape(2,3,4)\n",
    "pred_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lpe9JzvOIcFV",
    "outputId": "d8dd88ee-5fd4-48c5-bf1c-fc619b14b7f0"
   },
   "outputs": [],
   "source": [
    "print(np.argmax(pred_ids, axis =2)) #  greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZgAKip_gtGh"
   },
   "source": [
    "* max_steps = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XBc6uU1rJsIm"
   },
   "source": [
    "! 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "bRjrWUpawa27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Trainer을 활용하는 형태로 모델 재생성\n",
    "from transformers import Trainer, TrainingArguments\n",
    "output_dir = '/content/drive/MyDrive/Jiwon/transformers'\n",
    "metric_name = 'rouge'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir, # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\", #evaluation하는 빈도\n",
    "    learning_rate = 2e-5, #learning_rate\n",
    "    per_device_train_batch_size = 16, # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 16, # evaluation 시에 batch size\n",
    "    num_train_epochs = 5, # train 시킬 총 epochs\n",
    "    weight_decay = 0.01, # weight decay\n",
    "    max_steps = 500,\n",
    "    save_total_limit=5,\n",
    "    save_steps=500,\n",
    "    # load_best_model_at_end = True\n",
    "    # prediction_step = 100000,\n",
    "    # do_eval = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PGo8GOCmOSm2"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_metric, Dataset\n",
    "# # metric = load_metric('glue', 'mnli')\n",
    "\n",
    "# def compute_metrics(eval_pred):    \n",
    "#     predictions,labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "is8R3Mn05G5q"
   },
   "outputs": [],
   "source": [
    "val = Dataset.from_dict(encoded_data['test'][:320])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyosCrqIJomS"
   },
   "source": [
    " ! 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yp6bBD9uxHv3",
    "outputId": "1f7d1c59-b7fe-465d-b2b0-eb7611455491"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_gpt3,                           # 학습시킬 model\n",
    "    args=training_arguments,                  # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=encoded_data['train'],    # training dataset\n",
    "    eval_dataset=val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "yI9J9mgQcJ15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "# del encoded_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Sat Nov 12 07:16:43 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.172.01   Driver Version: 450.172.01   CUDA Version: 11.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   60C    P0    28W /  70W |   1298MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "XKK64-0kRwv3",
    "outputId": "bd4a3c0a-c743-4057-83c3-c7f33743eb0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: target, input. If target, input are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 15250\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 500\n",
      "  Number of trainable parameters = 118884864\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 13.21 GiB already allocated; 25.75 MiB free; 13.46 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m         )\n\u001b[0;32m-> 1501\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1502\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2538\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2539\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2540\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2541\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1047\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwte\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m         \u001b[0mposition_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwpe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mposition_embeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 14.76 GiB total capacity; 13.21 GiB already allocated; 25.75 MiB free; 13.46 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eva0LkFxKl4"
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate(encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "nA3pTno852FY",
    "outputId": "281b5af6-1425-4069-bf46-8d29a72b5b9d"
   },
   "outputs": [],
   "source": [
    "compute_metrics(encoded_data[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8q7jMQHLGry",
    "outputId": "b5aced9c-cb4b-4483-9e93-d37c15413adb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] 4일 오후 4시 충남 아산시 신리초등학교 체육관 2층 입구. 손 소독과 발열 체크를 한 뒤 체육관 안으로 들어가라는 안내문이 붙었다. 마스크를 착용하지 않으면 출입이 통제됐다. 체육관으로 들어가자 왼쪽에 설치된 대형 칠판에 [UNK] 사람 두 팔 간격 건강 거리 유지 [UNK] 라는 글이 크게 쓰여 있었다. 지난 3일 충남 아산지역에 폭우가 내리면서 온양천이 범람하자 신동 [UNK] 모종동 주민들이 급히 신리초로 대피했다. 신리초는 홍수와 지진 등 각종 재난사고 때 임시 주거시설로 지정된 곳이다. 3일 오후 5시까지 60여 명 수준이던 이재민은 오후 7시가 지나서는 100여 명을 넘어섰다. 아산시는 주민들이 한꺼번에 몰리자 수해와 더불어 신종 코로나바이러스 감염증 ( 코로나19 ) 이 확산할 것을 우려했다. 가장 먼저 주민들에게 마스크 착용을 의무화했다. 마스크를 쓰지 않으면 체육관 출입을 제한하고 체육관에 머무는 시간에도 밥을 먹거나 물을 마실 때를 제외하고 마스크를 벗지 못하게 했다. 이재민들은 집이나 농경지로 오갈 때도 출입 횟수에 관계없이 매번 발열 체크를 했다. 외부에서 다른 사람과 접촉할 가능성이 있어서다. 4일 오후 4시 30분쯤 체육관 안에는 주민 5명과 아산시청 공무원 [UNK] 자원봉사자 10여 명이 머물고 있었다. 주민들은 아산시가 제공한 매트리스 위에서 쉬거나 대화를 나눴다. 매트리스 1개마다 1명씩 배정됐다. 매트리스 간격은 2 ~ 3m가량으로 1개의 매트리스에 2명 이상이 앉지 못하도록 했다. 주민 간 접촉을 최소화하기 위해서다. 다른 대피소와 달리 신리초 체육관에는 소형 천막 ( 텐트 ) 이 지급되지 않았다. 이 때문에 충남도와 아산시 등 방역 당국이 바짝 긴장했다. 이곳에 머무르는 주민들은 대부분 70 ~ 80대 고령자로 코로나19에 노출될 경우 감염 위험이 높아서다. 아산시는 주민들이 식사를 할 때 밀접 접촉할 가능성이 크다고 판단, 모든 식사를 도시락으로 대체했다. 한 곳에 도시락을 쌓아 놓고 주민들이 줄을 서서 받지 않고 공무원들이 일일이 매트리스로 이동하며 도시락을 나눠줬다. 물도 각각 생수병을 전달하고 컵 사용도 자제할 것을 당부했다. 아산시 관계자는 [UNK] 이재민 대피시설에서 코로나19 확산 가능성이 있기 때문에 방역과 함께 철저한 관리가 이뤄지고 있다 [UNK] 며 [UNK] 마을회관과 경로당 등 소규모 인원이 모여 있는 곳도 담당 공무원이 수시로 확인한다 [UNK] 고 말했다. 지난 3일 새벽부터 4일 오후까지 내린 비로 충남에서는 620여 명 ( 364가구 ) 의 이재민이 발생했다. [SEP] [PAD]퓘촷퇏 뙑 [unused1700] [unused280]썖 [unused392] [unused824]쉕톽 럸 [unused864] [unused1511] [unused634] [unused1408] [unused351] [unused772] [unused573] 렊 [unused349] [unused1962] [unused1625] [unused505] [unused147] [unused138] [unused1058] [unused779] [unused1187] 쉕 [unused1271] [unused1053] [unused1923]\n"
     ]
    }
   ],
   "source": [
    "# text = '근육이 커지기 위해서는'\n",
    "input_ids = tokenizer_gpt3.encode(text, return_tensors='pt')\n",
    "\n",
    "model_gpt3.to('cpu')\n",
    "gen_ids = model_gpt3.generate(input_ids,\n",
    "                          max_length=512,\n",
    "                          repetition_penalty=2.0,\n",
    "                          pad_token_id=tokenizer_gpt3.pad_token_id,\n",
    "                          eos_token_id=tokenizer_gpt3.eos_token_id,\n",
    "                          bos_token_id=tokenizer_gpt3.bos_token_id,\n",
    "                          use_cache=True)\n",
    "generated = tokenizer_gpt3.decode(gen_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' [PAD]퓘촷퇏 뙑 [unused1700] [unused280]썖 [unused392] [unused824]쉕톽 럸 [unused864] [unused1511] [unused634] [unused1408] [unused351] [unused772] [unused573] 렊 [unused349] [unused1962] [unused1625] [unused505] [unused147] [unused138] [unused1058] [unused779] [unused1187] 쉕 [unused1271] [unused1053] [unused1923]']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated.split(tokenizer_gpt3.sep_token)[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[SEP]'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4일 오후 4시 충남 아산시 신리초등학교 체육관 2층 입구.\\n  손 소독과 발열 체크를 한 뒤 체육관 안으로 들어가라는 안내문이 붙었다.\\n  마스크를 착용하지 않으면 출입이 통제됐다.\\n  체육관으로 들어가자 왼쪽에 설치된 대형 칠판에 ‘사람 두 팔 간격 건강 거리 유지’라는 글이 크게 쓰여 있었다.\\n  지난 3일 충남 아산지역에 폭우가 내리면서 온양천이 범람하자 신동·모종동 주민들이 급히 신리초로 대피했다.\\n  신리초는 홍수와 지진 등 각종 재난사고 때 임시 주거시설로 지정된 곳이다.\\n  3일 오후 5시까지 60여 명 수준이던 이재민은 오후 7시가 지나서는 100여 명을 넘어섰다.\\n  아산시는 주민들이 한꺼번에 몰리자 수해와 더불어 신종 코로나바이러스 감염증(코로나19)이 확산할 것을 우려했다.\\n  가장 먼저 주민들에게 마스크 착용을 의무화했다.\\n  마스크를 쓰지 않으면 체육관 출입을 제한하고 체육관에 머무는 시간에도 밥을 먹거나 물을 마실 때를 제외하고 마스크를 벗지 못하게 했다.\\n  이재민들은 집이나 농경지로 오갈 때도 출입 횟수에 관계없이 매번 발열 체크를 했다.\\n  외부에서 다른 사람과 접촉할 가능성이 있어서다.\\n  4일 오후 4시 30분쯤 체육관 안에는 주민 5명과 아산시청 공무원·자원봉사자 10여 명이 머물고 있었다.\\n  주민들은 아산시가 제공한 매트리스 위에서 쉬거나 대화를 나눴다.\\n  매트리스 1개마다 1명씩 배정됐다.\\n  매트리스 간격은 2~3m가량으로 1개의 매트리스에 2명 이상이 앉지 못하도록 했다.\\n  주민 간 접촉을 최소화하기 위해서다.\\n  다른 대피소와 달리 신리초 체육관에는 소형 천막(텐트)이 지급되지 않았다.\\n  이 때문에 충남도와 아산시 등 방역 당국이 바짝 긴장했다.\\n  이곳에 머무르는 주민들은 대부분 70~80대 고령자로 코로나19에 노출될 경우 감염 위험이 높아서다.\\n  아산시는 주민들이 식사를 할 때 밀접 접촉할 가능성이 크다고 판단, 모든 식사를 도시락으로 대체했다.\\n  한 곳에 도시락을 쌓아 놓고 주민들이 줄을 서서 받지 않고 공무원들이 일일이 매트리스로 이동하며 도시락을 나눠줬다.\\n  물도 각각 생수병을 전달하고 컵 사용도 자제할 것을 당부했다.\\n  아산시 관계자는 “이재민 대피시설에서 코로나19 확산 가능성이 있기 때문에 방역과 함께 철저한 관리가 이뤄지고 있다”며 “마을회관과 경로당 등 소규모 인원이 모여 있는 곳도 담당 공무원이 수시로 확인한다”고 말했다.\\n  지난 3일 새벽부터 4일 오후까지 내린 비로 충남에서는 620여 명(364가구)의 이재민이 발생했다.\\n 충남 아산지역에 폭우가 내려 온양천이 범람하자 주민들은 신리초로 대피했고 이에 아산시는 코로나19 확산이 걱정되어 마스크를 쓰지 않으면 출입을 제한했다. '"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = encoded_data['train']['input'][0] + encoded_data['train']['target'][0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'충남 아산지역에 폭우가 내려 온양천이 범람하자 주민들은 신리초로 대피했고 이에 아산시는 코로나19 확산이 걱정되어 마스크를 쓰지 않으면 출입을 제한했다. '"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data['train']['target'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 15250\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcAkUIenZfEb"
   },
   "source": [
    "## (4)Generate_Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WK-KXxgRbdR3"
   },
   "source": [
    "* train_ptuning\n",
    "* KoGPTConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fzyGq6oc2jv",
    "outputId": "28740ed6-373d-4474-9352-c1d1f9910161"
   },
   "outputs": [],
   "source": [
    "! pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8EW-0I-sZi6d",
    "outputId": "b818dcae-0144-4741-ae58-50379ccb62be"
   },
   "outputs": [],
   "source": [
    "! git clone https://github.com/seujung/KoGPT2-summarization.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FsGR0uzRbk_o",
    "outputId": "2efb47b8-e06c-4a61-bc99-f44f3a333b85"
   },
   "outputs": [],
   "source": [
    "# ! pip install train_ptuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tse7m7b8dD1s",
    "outputId": "fee62acc-2710-49f6-d5e3-b526ceba9c35"
   },
   "outputs": [],
   "source": [
    "# ! pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lXOkuHnYaHiT",
    "outputId": "acfdb2d5-538b-4946-b652-f5514f4d388b"
   },
   "outputs": [],
   "source": [
    "%cd KoGPT2-summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1kajanCadRu"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('KoGPT2-summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82bS36rnah-a"
   },
   "outputs": [],
   "source": [
    "! pip install torch==1.8.1\n",
    "! pip install transformers==4.5.1\n",
    "! pip install pytorch-lightning==1.1.0\n",
    "! pip install streamlit==0.72.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tI6OeIFXeXi4"
   },
   "outputs": [],
   "source": [
    "!pip list | grep pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVhfdFzigM9S",
    "outputId": "e4cf09b5-d542-40c5-f036-2afdbbc3e26d"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZpJ_tA6qIlD"
   },
   "source": [
    "GPT2 generator > summmary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S69WCeEnZd7r"
   },
   "outputs": [],
   "source": [
    "\n",
    "import yaml\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import utils\n",
    "from train_ptuning import KoGPTConditionalGeneration\n",
    "from utils import generate_next_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNHni_6-bpHZ"
   },
   "outputs": [],
   "source": [
    "!mkdir log\n",
    "!touch log/hparams.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQOIcK9iasKI"
   },
   "outputs": [],
   "source": [
    "hparams_file = 'log/hparams.yaml'\n",
    "with open(hparams_file) as f:\n",
    "    hparams = yaml.load(f, yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1D0i-HtYjV7J"
   },
   "outputs": [],
   "source": [
    "!touch log/KoGPT2_summary-last.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "QQaf95YoawgX",
    "outputId": "75e9ed8c-a201-4cbd-fa68-d324912ccb03"
   },
   "outputs": [],
   "source": [
    "inf = KoGPTConditionalGeneration.load_from_checkpoint('./log/KoGPT2_summary-last.ckpt', hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vbDRVWP1a37H"
   },
   "outputs": [],
   "source": [
    "tokenizer = inf.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VFkdDQ5Fa8C6"
   },
   "outputs": [],
   "source": [
    "SUMMARY = ''\n",
    "PTUNING = ''\n",
    "EOS = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDERxDB1a_wk"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"수도권 지역 일일 코로나19 신규 확진자가 지난 3차 유행 이후 반년 만에 연일 600명대를 기록하면서 다음 주 수도권 지역에 새로운 사회적 거리두기 체계를 적용할 수 있을지 관심이 쏠리고 있다.\n",
    "\n",
    "젊은 층을 중심으로 확진자가 늘고 있고 인도에서 유래한 델타 변이 바이러스가 수도권 집단감염 사례에서 발견되는 등 유행 상황이 심상치 않다.\n",
    "\n",
    "감염병 전문가들은 상황이 나아질 때까지 새 거리두기 적용을 미뤄야 한다고 조언했다. 새 거리두기를 시행한다면 단계를 높여서 적용하거나 여름 휴가, 변이 유행 등을 대비해 방역 조처를 강화해야 한다고 강조했다.\n",
    "\n",
    "2일 질병관리청 중앙방역대책본부에 따르면 이날 0시 기준 국내 일일 코로나19 신규 확진자는 826명이다.\n",
    "\n",
    "지난달 30일과 이달 1일 연이틀 700명대로 집계된 데 이어 이날 800명대로 급증했다. 이는 3차 유행이 정점에 도달한 직후였던 1월7일 869명 이후 176일 만에 최대 규모다. 국내 발생 확진자 수도 같은 기간 최대 규모인 765명을 기록했다. 일주일간 하루 평균 환자 수는 635.6명으로, 이틀 연속 600명대에서 증가하고 있다.\n",
    "\n",
    "특히 수도권 지역 유행세가 크게 증가하고 있다. 수도권 지역 국내 발생 확진자는 지난달 30일 631명을 기록한 데 이어 이달 1일 607명, 이날 619명으로 집계됐다. 연일 600명대를 기록한 건 지난 1월7일 이후 처음이다.\n",
    "\n",
    "최근 수도권 지역에서 유행 증가세를 보이자 중앙재난안전대책본부(중대본)와 수도권 3개 시·도는 당초 1일 적용하려던 새 거리두기 체계 시행을 일주일 연기했다. 중대본과 수도권 지자체는 이번 주까지 유행 상황을 지켜본 후 다음 주 초에 논의를 거쳐 새 체계 적용 여부를 결정할 계획이다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dwlsp5ZoSTl",
    "outputId": "fed9f38d-a8e6-4d7d-d0bd-971b74e8779e"
   },
   "outputs": [],
   "source": [
    "tokenizer.encode(EOS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "BnkzwtVObC9Z",
    "outputId": "ef0dba0c-98bc-4bdd-a96a-1f91774cdf1a"
   },
   "outputs": [],
   "source": [
    "text = text.replace('\\n', '')\n",
    "input_tokens = tokenizer.encode(PTUNING)* 10 + tokenizer.encode(text) + tokenizer.encode(SUMMARY)\n",
    "input_tensor = torch.tensor(input_tokens).unsqueeze(0)\n",
    "\n",
    "eos_id = tokenizer.encode(EOS)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzQ6QWp-bGbe"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    pred = inf.model(input_tensor)\n",
    "    next_token = generate_next_token(pred.logits, temperature=1.0, top_p=0.8)\n",
    "\n",
    "    if next_token.item() == eos_id:\n",
    "        break\n",
    "    else:\n",
    "        input_tensor = torch.cat([input_tensor, next_token.unsqueeze(0)],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "__9HZqrTbJII"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(input_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daBAhNKFbLyM"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(input_tensor[0]).split('')[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PV1-VGKUHcf"
   },
   "source": [
    "# Daily Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJEOjpZCUK9L"
   },
   "source": [
    "## DAY02\n",
    "\n",
    "> Task1) Preprocessing\n",
    "\n",
    "\n",
    "> Task2) Checkpoint\n",
    "- [checkpoint](https://dacon.io/en/competitions/official/235875/codeshare/4520)\n",
    " * output_dir 설정 \n",
    " * save_total_limit\n",
    " * save_stepts\n",
    " * load_best_model_at_end = True\n",
    " * resum_from_checkpoint\n",
    "- [resume_from_checkpoint](https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/trainer#transformers.TrainingArguments)\n",
    " * Q1)`Resuming training from a checkpoint` can be done when calling Trainer.train() with either:\n",
    "   * `resume_from_checkpoint=True` which will resume training from the latest checkpoint\n",
    "   * `resume_from_checkpoint=checkpoint_dir` which will resume training from the specific checkpoint in the directory passed.\n",
    "\n",
    "> Task) Generate_Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKvwQnOkUK_z"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263,
     "referenced_widgets": [
      "42a96ad9d11e47e5a5908c7420e9f020",
      "43697c62a1ff4c2487d7d329fffc35cb",
      "d8a9fb10976247c59e5522ea2d4d8a01",
      "6f1873f76b674b639f4ad86a8468fb93",
      "08f7f7f93cf84564bc4e3cc40bbcfdf6",
      "459fe8147bf94f67a7267ceb977cb8a0",
      "2b34fcf6cfeb428abd1088a0a3d0bae6",
      "f6241600d51e4e8dbf2d72665b02924b",
      "358e581f7a4a40269ae6b886d9edfcc8",
      "c10a2c27ceab4d80abfb9890a5086838",
      "5acfa80091e946538cfdeac8aa09c09c"
     ]
    },
    "id": "NO1m4eeCUKdC",
    "outputId": "49faa662-d363-4689-959c-c53f5b631798"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ITCe10JvnMFu",
    "outputId": "961679a6-664f-438a-ddf1-0367c38b22f7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pred_str = tokenizer.batch_decode(np.argmax(logits.detach().numpy(), axis =2), skip_special_tokens=True)\n",
    "pred_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kplQ-tM6mRGt",
    "outputId": "0c736ce1-66e7-4294-bad1-bcc372c5f927"
   },
   "outputs": [],
   "source": [
    "outputs = model(inputs)\n",
    "logits = outputs.logits\n",
    "logits"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08f7f7f93cf84564bc4e3cc40bbcfdf6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2538659792a04f43830275f928f61e78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "27a07962ce3a4ab280746f9cee032621": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b34fcf6cfeb428abd1088a0a3d0bae6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3080a75425b14f37b5881f23f1b2035e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_585d1b3cd1a3406cbe36c7e26362e0cb",
      "placeholder": "​",
      "style": "IPY_MODEL_7d39bc464c2e445da1418d69c8bdf4ca",
      "value": " 3/3 [00:00&lt;00:00,  7.02it/s]"
     }
    },
    "358e581f7a4a40269ae6b886d9edfcc8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "42a96ad9d11e47e5a5908c7420e9f020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_43697c62a1ff4c2487d7d329fffc35cb",
       "IPY_MODEL_d8a9fb10976247c59e5522ea2d4d8a01",
       "IPY_MODEL_6f1873f76b674b639f4ad86a8468fb93"
      ],
      "layout": "IPY_MODEL_08f7f7f93cf84564bc4e3cc40bbcfdf6"
     }
    },
    "43697c62a1ff4c2487d7d329fffc35cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_459fe8147bf94f67a7267ceb977cb8a0",
      "placeholder": "​",
      "style": "IPY_MODEL_2b34fcf6cfeb428abd1088a0a3d0bae6",
      "value": "Downloading: 100%"
     }
    },
    "459fe8147bf94f67a7267ceb977cb8a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "585d1b3cd1a3406cbe36c7e26362e0cb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5acfa80091e946538cfdeac8aa09c09c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f1873f76b674b639f4ad86a8468fb93": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c10a2c27ceab4d80abfb9890a5086838",
      "placeholder": "​",
      "style": "IPY_MODEL_5acfa80091e946538cfdeac8aa09c09c",
      "value": " 548M/548M [00:25&lt;00:00, 39.4MB/s]"
     }
    },
    "7b41892ccc7743a2afcd550a8e3b57d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c13a315ca033475ba62823805a462486",
      "placeholder": "​",
      "style": "IPY_MODEL_8c36d30ca30b4eeb85842e02c5933194",
      "value": "100%"
     }
    },
    "7d39bc464c2e445da1418d69c8bdf4ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c36d30ca30b4eeb85842e02c5933194": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "90e9236dcfc34195835948daf3a5d6b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7b41892ccc7743a2afcd550a8e3b57d1",
       "IPY_MODEL_d8d0841a119b434a841d897e13adaf92",
       "IPY_MODEL_3080a75425b14f37b5881f23f1b2035e"
      ],
      "layout": "IPY_MODEL_27a07962ce3a4ab280746f9cee032621"
     }
    },
    "a20b879929f14131ba4a5e5a9fc05807": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c10a2c27ceab4d80abfb9890a5086838": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c13a315ca033475ba62823805a462486": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8a9fb10976247c59e5522ea2d4d8a01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6241600d51e4e8dbf2d72665b02924b",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_358e581f7a4a40269ae6b886d9edfcc8",
      "value": 548118077
     }
    },
    "d8d0841a119b434a841d897e13adaf92": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a20b879929f14131ba4a5e5a9fc05807",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2538659792a04f43830275f928f61e78",
      "value": 3
     }
    },
    "f6241600d51e4e8dbf2d72665b02924b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
