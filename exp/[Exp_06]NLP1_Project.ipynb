{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_4e1bCjWHa8"
      },
      "source": [
        "#[Exp-06]NLP_Task1 : 작사작곡"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STh49IGoMxuD",
        "outputId": "11c5a85b-f5cd-4fd7-aea4-3d7ea6467178"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLK7rDA_UpTB"
      },
      "source": [
        "# 목차\n",
        "\n",
        "project1: 멋진 작사가 만들기 \n",
        "- 1.데이터 다운로드\n",
        "- 2.데이터 읽어오기\n",
        "- 3.데이터 정제\n",
        "- 4.평가 데이터셋 분리\n",
        "- 5.인공지능 만들기 \n",
        "\n",
        "\n",
        "회고"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI4iQAzuWKwM"
      },
      "source": [
        "# 1.라이브러리 버전 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vqoF9CcWObI",
        "outputId": "373b0b2c-4b21-4502-ae13-beae2fee6d0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "\n",
        "print(tensorflow.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK4Cj4ZUWQBc"
      },
      "source": [
        "#2.데이터 읽어오기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4qyUE4nZM2g",
        "outputId": "ed8278ef-44b5-434c-b25c-bc01a9dfa2ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/alicia-keys.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/al-green.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bjork.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/beatles.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/blink-182.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bieber.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/amy-winehouse.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/britney-spears.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bruce-springsteen.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bruno-mars.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bob-marley.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/disney.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/cake.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/dickinson.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/drake.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/dolly-parton.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/dr-seuss.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/lin-manuel-miranda.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/dj-khaled.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/Kanye_West.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/patti-smith.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/rihanna.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/paul-simon.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/r-kelly.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/prince.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/nirvana.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/notorious-big.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/notorious_big.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/nickelback.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/nicki-minaj.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/ludacris.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/michael-jackson.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/lorde.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/missy-elliott.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/lady-gaga.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/Lil_Wayne.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/kanye.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/leonard-cohen.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/lil-wayne.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/janisjoplin.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/jimi-hendrix.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/eminem.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/joni-mitchell.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/kanye-west.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/johnny-cash.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/radiohead.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/bob-dylan.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/adele.txt', '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/nursery_rhymes.txt']\n",
            "데이터 크기: 187088\n",
            "Examples:\n",
            " ['Ooh....... New York x2 Grew up in a town that is famous as a place of movie scenes', 'Noise is always loud, there are sirens all around and the streets are mean', \"If I can make it here, I can make it anywhere, that's what they say\"]\n"
          ]
        }
      ],
      "source": [
        "import os, re, glob \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "txt_file_path = '/content/drive/MyDrive/Aiffel_data/Exp6-composition/project/*'\n",
        "\n",
        "txt_list = glob.glob(txt_file_path)\n",
        "\n",
        "print(txt_list)\n",
        "\n",
        "raw_corpus = []\n",
        "\n",
        "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
        "for txt_file in txt_list:\n",
        "    with open(txt_file, \"r\") as f:\n",
        "        raw = f.read().splitlines()\n",
        "        raw_corpus.extend(raw)\n",
        "\n",
        "print(\"데이터 크기:\", len(raw_corpus))\n",
        "print(\"Examples:\\n\", raw_corpus[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-M1g0wJWQF4"
      },
      "source": [
        "#3.데이터 정제"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCPPs3nyccSH",
        "outputId": "37dee0fb-4b65-43c8-985c-63fb38b37aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ooh....... New York x2 Grew up in a town that is famous as a place of movie scenes\n",
            "Ooh....... New York x2 Grew up in a town that is famous as a place of movie scenes\n",
            "Noise is always loud, there are sirens all around and the streets are mean\n",
            "Noise is always loud, there are sirens all around and the streets are mean\n",
            "If I can make it here, I can make it anywhere, that's what they say\n",
            "If I can make it here, I can make it anywhere, that's what they say\n",
            "Seeing my face in lights or my name on marquees found down on Broadway Even if it ain't all it seems, I got a pocket full of dreams\n",
            "Seeing my face in lights or my name on marquees found down on Broadway Even if it ain't all it seems, I got a pocket full of dreams\n",
            "Baby, I'm from New York\n",
            "Baby, I'm from New York\n",
            "Concrete jungle where dreams are made of\n",
            "Concrete jungle where dreams are made of\n",
            "There's nothing you can't do\n",
            "There's nothing you can't do\n",
            "Now you're in New York\n",
            "Now you're in New York\n",
            "These streets will make you feel brand new\n",
            "These streets will make you feel brand new\n",
            "Big lights will inspire you\n",
            "Big lights will inspire you\n",
            "Hear it from New York, New York, New York! On the avenue, there ain't never a curfew, ladies work so hard\n"
          ]
        }
      ],
      "source": [
        "for idx, sentence in enumerate(raw_corpus):\n",
        "\n",
        "    print(sentence)\n",
        "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뜁니다.\n",
        "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
        "\n",
        "    if idx > 9: break   # 일단 문장 10개만 확인해 볼 겁니다.\n",
        "        \n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXLDchcjf7Y_",
        "outputId": "663efe22-6c8f-494a-e75b-2b7da88efe08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 입력된 문장을\n",
        "# 01. 양쪽 공백 제거\n",
        "# 02. 여러개 공백 하나로 만들기\n",
        "# 03. ()[] 제거 특수문자 제거\n",
        "# 04. 모든 문자 대문자 \n",
        "def preprocess_sentence(sentence):\n",
        "    setentce = sentence.strip()\n",
        "    sentence = re.sub('[\" \"]+', \" \", sentence) \n",
        "    sentence = re.sub(\"[^a-zA-Z \\'(\\)\\[\\]]+\", \"\", sentence) \n",
        "    sentence = re.sub(\"\\[[\\w\\d\\s-]*\\]\", \"\", sentence)\n",
        "    sentence = re.sub(\"\\([\\w\\d\\s-]*\\)\", \"\", sentence)\n",
        "    if (str.isupper(sentence) ==True):\n",
        "      sentence = \"\"\n",
        "    sentence = sentence.lower().strip()\n",
        "    if len(sentence) != 0:\n",
        "      sentence = '<start> ' + sentence + ' <end>' \n",
        "\n",
        "    sentence = sentence.lower().strip()\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 이 문장이 어떻게 필터링되는지 확인해 보세요.\n",
        "print(preprocess_sentence(\"UPPER NO UPPER\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDxt1GcRfmQp",
        "outputId": "f8b017fc-b8f6-4bc0-c52f-1f138b416420"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"<start> baby i'm from new york <end>\",\n",
              " '<start> concrete jungle where dreams are made of <end>',\n",
              " \"<start> there's nothing you can't do <end>\",\n",
              " \"<start> now you're in new york <end>\",\n",
              " '<start> these streets will make you feel brand new <end>',\n",
              " '<start> big lights will inspire you <end>',\n",
              " '<start> such a melting pot on the corner selling rock preachers pray to god <end>',\n",
              " '<start> hail a gypsycab takes me down from harlem to the brooklyn bridge <end>',\n",
              " \"<start> baby i'm from new york <end>\",\n",
              " '<start> concrete jungle where dreams are made of <end>']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 여기에 정제된 문장을 모을겁니다\n",
        "corpus = []\n",
        "\n",
        "for sentence in raw_corpus:\n",
        "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
        "    if len(sentence) == 0: continue\n",
        "    if sentence[-1] == \"+\": continue\n",
        "    if sentence[-1] == \"-\": continue\n",
        "    \n",
        "    # 정제를 하고 담아주세요\n",
        "    preprocessed_sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    if len(preprocessed_sentence) == 0: continue \n",
        "    if preprocessed_sentence == '<start> the queen of hearts <end>' : continue\n",
        "    if len(preprocessed_sentence.split()) > 15 : continue    \n",
        "\n",
        "\n",
        "    corpus.append(preprocessed_sentence)\n",
        "\n",
        "\n",
        "        \n",
        "# 정제된 결과를 10개만 확인해보죠\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKq5Vav-fmpQ",
        "outputId": "42324693-8ed9-40ea-cdbb-86c8375b9cb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[   2   44   18 ...    0    0    0]\n",
            " [   2 2836 2011 ...    0    0    0]\n",
            " [   2  162  174 ...    0    0    0]\n",
            " ...\n",
            " [   2    7   42 ...    0    0    0]\n",
            " [   2   97   86 ...    0    0    0]\n",
            " [   2    7   42 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7fa6d43a9cd0>\n"
          ]
        }
      ],
      "source": [
        "# 토큰화 할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용합니다\n",
        "\n",
        "def tokenize(corpus):\n",
        "    # 12000개 기억할 수 있는 tokenizer를 만들겁니다\n",
        "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
        "    # 7000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=12000, \n",
        "        filters=' ',\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "\n",
        "    # corpus를 이용해 tokenizer 내부의 '단어장'을 완성합니다\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
        "    tensor = tokenizer.texts_to_sequences(corpus) \n",
        "\n",
        "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
        "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
        "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  \n",
        "    \n",
        "    print(tensor,tokenizer)\n",
        "    return tensor, tokenizer\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 단어 빈도수 \n",
        "# for k, v in tokenizer.word_counts.items():\n",
        "#   if len(k) == 1:\n",
        "#     print(k, v)\n",
        "# print(tokenizer.word_counts.items())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmyPF7JFg9ne",
        "outputId": "0093a34e-7910-42f9-9b05-6cd2afcb4a9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  2  44  18  63 151 710   3   0   0   0   0   0   0   0]\n",
            "[ 44  18  63 151 710   3   0   0   0   0   0   0   0   0]\n"
          ]
        }
      ],
      "source": [
        "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성합니다\n",
        "\n",
        "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높습니다.\n",
        "src_input = tensor[:, :-1]  \n",
        "\n",
        "# tensor에서 <start>를 잘라내서 타겟 문장을 생성합니다.\n",
        "tgt_input = tensor[:, 1:]    \n",
        "\n",
        "print(src_input[0])\n",
        "print(tgt_input[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5dssc71WQJB"
      },
      "source": [
        "#4.평가 데이터셋 분리\n",
        "\n",
        "tokenize() 함수로 데이터를 Tensor로 변환한 후, sklearn 모듈의 train_test_split() 함수를 사용해 훈련 데이터와 평가 데이터를 분리하도록 하겠습니다. 단어장의 크기는 12,000 이상 으로 설정하세요! 총 데이터의 20% 를 평가 데이터셋으로 사용해 주세요!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_QcDqeycbK9",
        "outputId": "8f39eefc-20a0-4179-bf0d-0643a8f2cc0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "enc_train 개수:  128212 , enc_val 개수:  32053\n"
          ]
        }
      ],
      "source": [
        "#enc_train, enc_val, dec_train, dec_val = # <코드 작성>\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                    tgt_input, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42,\n",
        "                                                    )\n",
        "\n",
        "print('enc_train 개수: ', len(enc_train),', enc_val 개수: ', len(enc_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdsrLG4ehJLx",
        "outputId": "b06d2ee5-8cad-468f-d719-7c515abc0529"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(256, 14), dtype=tf.int32, name=None), TensorSpec(shape=(256, 14), dtype=tf.int32, name=None))>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "BUFFER_SIZE = len(src_input)\n",
        "BATCH_SIZE = 256\n",
        "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
        "\n",
        " # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개\n",
        "VOCAB_SIZE = tokenizer.num_words + 1   \n",
        "\n",
        "# 준비한 데이터 소스로부터 데이터셋을 만듭니다\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7C7KxWtiXPGu"
      },
      "source": [
        "#5.인공지능 만들기\n",
        "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UAxhhfPVDKQ_"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "    def call(self, x):\n",
        "        out = self.embedding(x)\n",
        "        out = self.rnn_1(out)\n",
        "        out = self.rnn_2(out)\n",
        "        out = self.linear(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "embedding_size = 256\n",
        "hidden_size = 2048\n",
        "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlcSP49bDZnx",
        "outputId": "514327c8-7c76-47e9-b607-6cab466eabbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
              "array([[[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [-2.51840684e-04, -8.65907423e-05, -1.40787932e-04, ...,\n",
              "         -2.38995999e-04, -4.17903735e-04,  1.90494422e-04],\n",
              "        [-5.48079726e-04, -3.28379829e-04, -2.12942381e-04, ...,\n",
              "         -5.10018872e-05, -4.86177480e-04,  2.95930309e-04],\n",
              "        ...,\n",
              "        [-1.49172742e-03, -9.49848676e-04, -4.18400305e-04, ...,\n",
              "          3.04138870e-04, -8.12267826e-04, -3.67788947e-04],\n",
              "        [-1.70879776e-03, -1.35561905e-03, -4.58778173e-04, ...,\n",
              "          3.79180623e-04, -1.08084606e-03, -4.20254975e-04],\n",
              "        [-1.92951597e-03, -1.74995116e-03, -5.17118431e-04, ...,\n",
              "          4.62063588e-04, -1.33457361e-03, -4.33335779e-04]],\n",
              "\n",
              "       [[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [ 4.19863791e-05,  2.49771576e-04, -4.00170800e-04, ...,\n",
              "         -3.24107707e-04, -4.87212628e-06,  1.51363580e-04],\n",
              "        [ 2.86419003e-04,  3.88104818e-04, -7.58128939e-04, ...,\n",
              "         -4.93856962e-04,  4.44489706e-05,  1.23114922e-04],\n",
              "        ...,\n",
              "        [-9.02807282e-04, -4.30341868e-04,  4.73718013e-04, ...,\n",
              "          5.47875359e-04, -4.66350408e-04, -7.20225100e-04],\n",
              "        [-1.31917384e-03, -8.92429380e-04,  4.55833098e-04, ...,\n",
              "          6.71278569e-04, -6.30843802e-04, -8.60418717e-04],\n",
              "        [-1.70962187e-03, -1.38215348e-03,  3.64026055e-04, ...,\n",
              "          7.75551132e-04, -8.27853219e-04, -9.37057310e-04]],\n",
              "\n",
              "       [[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [-2.91759527e-04, -5.33788581e-04, -1.77257651e-04, ...,\n",
              "         -2.52088867e-05, -3.39506114e-05,  4.10107255e-04],\n",
              "        [-3.08774965e-04, -5.61046298e-04, -3.73170158e-04, ...,\n",
              "          5.77458886e-05,  2.07251389e-04,  7.07088562e-04],\n",
              "        ...,\n",
              "        [-2.15721992e-03, -1.89099577e-03, -4.91859915e-04, ...,\n",
              "          1.52902852e-03, -8.54263781e-04, -4.00546342e-05],\n",
              "        [-2.42731837e-03, -2.27511954e-03, -6.03523280e-04, ...,\n",
              "          1.49449764e-03, -1.05151953e-03, -1.25634833e-04],\n",
              "        [-2.65984749e-03, -2.58890865e-03, -7.10582186e-04, ...,\n",
              "          1.44617842e-03, -1.23265700e-03, -1.69143896e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [-2.04256939e-04, -5.65414230e-05, -3.17609200e-04, ...,\n",
              "         -7.44996214e-05, -1.55563583e-04,  2.66744202e-04],\n",
              "        [-2.74911610e-04, -1.15819545e-04, -3.66815948e-05, ...,\n",
              "          4.67985155e-05, -2.92176963e-04,  2.36377353e-04],\n",
              "        ...,\n",
              "        [-1.91482017e-03, -1.03929173e-03, -8.44803173e-04, ...,\n",
              "          1.77492693e-04, -9.37960111e-04, -4.26537561e-04],\n",
              "        [-2.14670459e-03, -1.45823217e-03, -9.04404791e-04, ...,\n",
              "          3.36589932e-04, -1.13231654e-03, -4.42473887e-04],\n",
              "        [-2.35419045e-03, -1.82172447e-03, -9.53031937e-04, ...,\n",
              "          4.57761547e-04, -1.30302762e-03, -4.24905360e-04]],\n",
              "\n",
              "       [[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [-1.22229496e-04,  2.04408527e-04, -3.25207424e-04, ...,\n",
              "         -7.34433925e-05, -1.97353686e-04,  3.18801060e-04],\n",
              "        [-3.44931352e-04,  4.54218505e-04, -1.55906251e-04, ...,\n",
              "         -2.77470419e-04, -2.46382057e-04,  4.06609150e-04],\n",
              "        ...,\n",
              "        [-2.21627508e-03, -1.67472020e-03, -3.28389957e-04, ...,\n",
              "          3.22183041e-04, -1.09270692e-03, -4.63580975e-04],\n",
              "        [-2.44435016e-03, -2.04623584e-03, -4.75486595e-04, ...,\n",
              "          4.04999068e-04, -1.24357862e-03, -4.30064596e-04],\n",
              "        [-2.65214732e-03, -2.34901300e-03, -6.06096524e-04, ...,\n",
              "          4.66789759e-04, -1.37842668e-03, -3.81650112e-04]],\n",
              "\n",
              "       [[-1.04436796e-04, -2.93278972e-05, -9.07612775e-05, ...,\n",
              "         -7.65805671e-05, -4.48094434e-05,  1.78937116e-04],\n",
              "        [ 4.46444537e-05, -9.94360598e-05, -7.43995406e-05, ...,\n",
              "          9.64696665e-06, -2.85118935e-04,  3.53857089e-04],\n",
              "        [ 2.72326812e-04, -5.93148361e-05, -1.84433113e-04, ...,\n",
              "         -3.50586139e-04, -3.39646736e-04,  5.52657642e-04],\n",
              "        ...,\n",
              "        [-2.22422695e-03, -2.14775349e-03, -9.73796821e-04, ...,\n",
              "          3.58818623e-04, -9.53174720e-04, -1.54150126e-04],\n",
              "        [-2.52144062e-03, -2.36475538e-03, -1.03002437e-03, ...,\n",
              "          4.88740567e-04, -1.09156338e-03, -1.53751782e-04],\n",
              "        [-2.77995970e-03, -2.53815693e-03, -1.06801908e-03, ...,\n",
              "          5.81933593e-04, -1.21916621e-03, -1.37654482e-04]]],\n",
              "      dtype=float32)>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 데이터셋에서 데이터 한 배치만 불러오는 방법입니다.\n",
        "\n",
        "for src_sample, tgt_sample in dataset.take(1): break\n",
        "\n",
        "# 한 배치만 불러온 데이터를 모델에 넣어봅니다\n",
        "model(src_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P07nr-pDSYB",
        "outputId": "5e103e6b-f20d-4582-fd4c-bb2f9741bb60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"text_generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  3072256   \n",
            "                                                                 \n",
            " lstm (LSTM)                 multiple                  18882560  \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               multiple                  33562624  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  24590049  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 80,107,489\n",
            "Trainable params: 80,107,489\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4Y0F_Io35vg",
        "outputId": "70732566-9e81-41bb-fc48-a51575ce5d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "500/500 [==============================] - 137s 269ms/step - loss: 3.3773 - val_loss: 2.9795\n",
            "Epoch 2/10\n",
            "500/500 [==============================] - 134s 269ms/step - loss: 2.8124 - val_loss: 2.7312\n",
            "Epoch 3/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.5394 - val_loss: 2.5660\n",
            "Epoch 4/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.2716 - val_loss: 2.4367\n",
            "Epoch 5/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 2.0146 - val_loss: 2.3370\n",
            "Epoch 6/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 1.7765 - val_loss: 2.2679\n",
            "Epoch 7/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 1.5656 - val_loss: 2.2186\n",
            "Epoch 8/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 1.3882 - val_loss: 2.1939\n",
            "Epoch 9/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 1.2459 - val_loss: 2.1792\n",
            "Epoch 10/10\n",
            "500/500 [==============================] - 134s 268ms/step - loss: 1.1401 - val_loss: 2.1922\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa6d1b99050>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Loss\n",
        "#모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요!\n",
        "\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none'\n",
        ")\n",
        "model.compile(loss=loss, optimizer=optimizer)\n",
        "\n",
        "model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rCUx62AcPWgo"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
        "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다\n",
        "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
        "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
        "    end_token = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    # 단어 하나씩 예측해 문장을 만듭니다\n",
        "    #    1. 입력받은 문장의 텐서를 입력합니다\n",
        "    #    2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
        "    #    3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
        "    #    4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
        "    while True:\n",
        "        # 1\n",
        "        predict = model(test_tensor) \n",
        "        # 2\n",
        "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
        "        # 3 \n",
        "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
        "        # 4\n",
        "        if predict_word.numpy()[0] == end_token: break\n",
        "        if test_tensor.shape[1] >= max_len: break\n",
        "\n",
        "    generated = \"\"\n",
        "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
        "    for word_index in test_tensor[0].numpy():\n",
        "        generated += tokenizer.index_word[word_index] + \" \"\n",
        "\n",
        "    return generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "01b5SZjE3_j0",
        "outputId": "2525332b-668b-4357-9c30-7a9ae47ab436"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> you know that you need a rider <end> '"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_text(model, tokenizer, init_sentence=\"<start> you \", max_len=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5In_mI0ZMkuE"
      },
      "source": [
        "\n",
        "# 회고\n",
        "\n",
        "### - 이번 프로젝트에서 **어려웠던 점**.\n",
        "* 정규표현식 개념이 익숙하지 않아 데이터 정제시에 힘들었지만 새로 배우게 되어 좋은 기회였다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### - 프로젝트를 진행하면서 **알아낸 점** 혹은 **아직 모호한 점**.\n",
        "\n",
        "**{알아낸 점}**\n",
        "1. 정규표현식 사용 <정규표현식 유튜브 참고>\n",
        "* https://youtu.be/t3M6toIflyQ\n",
        "* https://youtu.be/dTDoTR0MXjU\n",
        "\n",
        "\n",
        "    sentence = re.sub(\"[^a-zA-Z \\'(\\)\\[\\]]+\", \"\", sentence) \n",
        "    sentence = re.sub(\"\\[[\\w\\d\\s-]*\\]\", \"\", sentence)\n",
        "    sentence = re.sub(\"\\([\\w\\d\\s-]*\\)\", \"\", sentence)\n",
        "\n",
        "2. split_train_test 을 응용해서 train _validation 셋으로 분리하기\n",
        "\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
        "                                                    tgt_input, \n",
        "                                                    test_size=0.2, \n",
        "                                                    random_state=42,\n",
        "                                                    )\n",
        "\n",
        "\n",
        "3. validation loss 설정 하기 \n",
        "\n",
        ":tensorflow fit 다큐먼트 검색\n",
        "\n",
        "    model.fit(dataset, epochs=10, validation_data=(enc_val, dec_val))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### - 루브릭 평가 지표를 맞추기 위해 **시도한 것들**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ">#### **루브릭평가 지표**\n",
        ">|번호|평가문항|상세기준|\n",
        ">|:---:|---|---|\n",
        ">|1|  가사 텍스트 생성 모델이 정상적으로 동작한다. |텍스트 제너레이션 결과가 그럴듯한 문장으로 생성되었다.|\n",
        ">|2|데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었다.| 특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었다.|\n",
        ">|3|텍스트 생성모델이 안정적으로 학습되었다.|텍스트 생성모델의 validation loss가 2.2 이하로 낮아졌다.|\n",
        "\n",
        ">  **hidden_size = 1024->2048 로 조정했습니다.**\n",
        "\n",
        "\n",
        "\n",
        "### - **자기 다짐**\n",
        "\n",
        "* 정규표현식 익숙해지게 공부해보기\n",
        "\n",
        "### - **참고**\n",
        "\n",
        "RNN Youtube : https://www.youtube.com/watch?v=NHb6jNH2YTg\n",
        "\n",
        "Tensor : https://rekt77.tistory.com/102\n",
        "\n",
        "RNN : https://nicola-ml.tistory.com/107\n",
        "\n",
        "fit_on_texts : https://www.dinolabs.ai/186\n",
        "\n",
        "Reg : https://blog.naver.com/ekqls9960/222488864305\n",
        "\n",
        "Super() : https://rednooby.tistory.com/56\n",
        "\n",
        "Tensor_Loss: https://hwiyong.tistory.com/335"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKb4to1KPGfl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "[Exp-6]NLP1_Project",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
